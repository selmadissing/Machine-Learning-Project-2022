{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Machine Learning Project 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links:\n",
    "\n",
    "https://towardsdatascience.com/natural-language-processing-nlp-for-machine-learning-d44498845d5b\n",
    "https://www.andyfitzgeraldconsulting.com/writing/keyword-extraction-nlp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 31,
>>>>>>> ce2ec3a (This is the newest version of wrapper)
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import re\n",
    "from nltk.probability import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "# from term_frequency import term_frequencies, feature_names, df_term_frequencies\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "transformer = TfidfTransformer()\n",
    "tt = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 32,
>>>>>>> ce2ec3a (This is the newest version of wrapper)
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 33,
>>>>>>> ce2ec3a (This is the newest version of wrapper)
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 34,
>>>>>>> ce2ec3a (This is the newest version of wrapper)
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
<<<<<<< HEAD
     "execution_count": 6,
=======
     "execution_count": 34,
>>>>>>> ce2ec3a (This is the newest version of wrapper)
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'train.csv')\n",
    "\n",
    "df.replace('NaN', np.NaN, inplace = True)\n",
    "\n",
    "# to count the number of NaN's in each column, just change the column name in this line to see how many missing values of that\n",
    "# variable per other column\n",
    "# print(df[df.keyword.isnull()].count())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 35,
>>>>>>> ce2ec3a (This is the newest version of wrapper)
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tweet_noNoise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                      tweet_noNoise  \n",
       "0       1  our deeds are the reason of this earthquake ma...  \n",
       "1       1              forest fire near la ronge sask canada  \n",
       "2       1  all residents asked to shelter in place are be...  \n",
       "3       1  13000 people receive wildfires evacuation orde...  \n",
       "4       1  just got sent this photo from ruby alaska as s...  "
      ]
     },
<<<<<<< HEAD
     "execution_count": 7,
=======
     "execution_count": 35,
>>>>>>> ce2ec3a (This is the newest version of wrapper)
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_tweets(text):\n",
    "    \n",
    "# remove mentions and URLs\n",
    "    text_noMentionURL = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", text)\n",
    "    text_noMentionURL = \" \".join(text_noMentionURL.split())\n",
    "    \n",
    "# remove '#' symbols and add space before capital letter\n",
    "    text_noHash = re.sub(r\"([A-Z]+)\", r\" \\1\", text)\n",
    "    text_noHash = re.sub(r\"(#)\", \"\", text_noHash)\n",
    "    text_noHash = \" \".join(text_noHash.split())\n",
    "    \n",
    "# remove all other punctuation\n",
    "    text_noNoise = \"\".join([char for char in text_noHash if char not in string.punctuation])\n",
    "\n",
    "    return text_noNoise.lower()\n",
    "\n",
    "\n",
    "df['tweet_noNoise'] = df[\"text\"].apply(lambda x: preprocess_tweets(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 36,
>>>>>>> ce2ec3a (This is the newest version of wrapper)
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tweet_noNoise</th>\n",
       "      <th>lemmatized_list</th>\n",
       "      <th>processed_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>[deed, reason, earthquake, may, allah, forgive...</td>\n",
       "      <td>deed,reason,earthquake,may,allah,forgive,u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>forest,fire,near,la,ronge,sask,canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>[resident, ask, shelter, place, notify, office...</td>\n",
       "      <td>resident,ask,shelter,place,notify,officer,evac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>[13000, people, receive, wildfire, evacuation,...</td>\n",
       "      <td>13000,people,receive,wildfire,evacuation,order...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>[get, send, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "      <td>get,send,photo,ruby,alaska,smoke,wildfire,pour...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                      tweet_noNoise  \\\n",
       "0       1  our deeds are the reason of this earthquake ma...   \n",
       "1       1              forest fire near la ronge sask canada   \n",
       "2       1  all residents asked to shelter in place are be...   \n",
       "3       1  13000 people receive wildfires evacuation orde...   \n",
       "4       1  just got sent this photo from ruby alaska as s...   \n",
       "\n",
       "                                     lemmatized_list  \\\n",
       "0  [deed, reason, earthquake, may, allah, forgive...   \n",
       "1      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2  [resident, ask, shelter, place, notify, office...   \n",
       "3  [13000, people, receive, wildfire, evacuation,...   \n",
       "4  [get, send, photo, ruby, alaska, smoke, wildfi...   \n",
       "\n",
       "                                     processed_tweet  \n",
       "0         deed,reason,earthquake,may,allah,forgive,u  \n",
       "1              forest,fire,near,la,ronge,sask,canada  \n",
       "2  resident,ask,shelter,place,notify,officer,evac...  \n",
       "3  13000,people,receive,wildfire,evacuation,order...  \n",
       "4  get,send,photo,ruby,alaska,smoke,wildfire,pour...  "
      ]
     },
<<<<<<< HEAD
     "execution_count": 8,
=======
     "execution_count": 36,
>>>>>>> ce2ec3a (This is the newest version of wrapper)
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatizer with Part-of-Speech (POS)\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "\n",
    "# this function can be called within lemmatize() to tag the word with its POS\n",
    "def get_pos(word):\n",
    "    probable_part_of_speech = wordnet.synsets(word)\n",
    "  \n",
    "    pos_counts = Counter()\n",
    "\n",
    "    pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "    pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "    pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "    pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "  \n",
    "    most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "    return most_likely_part_of_speech\n",
    "\n",
    "# tokenize tweets = divide into words\n",
    "tokenized_tweets = df.apply(lambda row: tt.tokenize(row['tweet_noNoise']), axis=1)\n",
    "\n",
    "# lemmatize the list of all tokens\n",
    "df['lemmatized_list'] = tokenized_tweets.apply(lambda y: [lemmatizer.lemmatize(x, get_pos(x)) for x in y])\n",
    "df['lemmatized_list'].apply(lambda x: ' '.join([tweet for tweet in x]))\n",
    "\n",
    "# removes stop words (are, on, in, etc.) MUST COME BEFORE LIST -> STR\n",
    "stop = set(stopwords.words('english'))\n",
    "df['lemmatized_list'] = df['lemmatized_list'].apply(lambda x: [y for y in x if y not in stop])\n",
    "\n",
    "# turn the resulting list back into a string\n",
    "df['processed_tweet'] = df.lemmatized_list.agg(lambda x: ','.join(map(str, x)))\n",
    "\n",
    "# remove weird characters and 1 character after\n",
    "df['processed_tweet'] = df['processed_tweet'].replace({'û.':''}, regex=True)\n",
    "df['processed_tweet'] = df['processed_tweet'].replace({'ì.':''}, regex=True)\n",
    "df['processed_tweet'] = df['processed_tweet'].replace({'å.':''}, regex=True)\n",
    "df['processed_tweet'] = df['processed_tweet'].replace({'â.':''}, regex=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 37,
>>>>>>> ce2ec3a (This is the newest version of wrapper)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weed\n"
     ]
    }
   ],
   "source": [
    "# enter whatever word you'd like to see how lemmatizing works\n",
    "print(lemmatizer.lemmatize('weeds'))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet 1</th>\n",
       "      <th>Tweet 2</th>\n",
       "      <th>Tweet 3</th>\n",
       "      <th>Tweet 4</th>\n",
       "      <th>Tweet 5</th>\n",
       "      <th>Tweet 6</th>\n",
       "      <th>Tweet 7</th>\n",
       "      <th>Tweet 8</th>\n",
       "      <th>Tweet 9</th>\n",
       "      <th>Tweet 10</th>\n",
       "      <th>...</th>\n",
       "      <th>Tweet 7604</th>\n",
       "      <th>Tweet 7605</th>\n",
       "      <th>Tweet 7606</th>\n",
       "      <th>Tweet 7607</th>\n",
       "      <th>Tweet 7608</th>\n",
       "      <th>Tweet 7609</th>\n",
       "      <th>Tweet 7610</th>\n",
       "      <th>Tweet 7611</th>\n",
       "      <th>Tweet 7612</th>\n",
       "      <th>Tweet 7613</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>001</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0011</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>001116</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0025</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>005225</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>007</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00c</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00kj</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>010156</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>010217</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0104</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.55145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>010401</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0106</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0111</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>012</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>012032</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>012624</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>014</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 7613 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Tweet 1  Tweet 2  Tweet 3  Tweet 4  Tweet 5  Tweet 6  Tweet 7  \\\n",
       "0000        0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "001         0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "0011        0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "001116      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "0025        0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "005225      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "007         0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "00c         0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "00kj        0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "01          0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "010156      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "010217      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "0104        0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "010401      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "0106        0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "0111        0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "012         0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "012032      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "012624      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "014         0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "        Tweet 8  Tweet 9  Tweet 10  ...  Tweet 7604  Tweet 7605  Tweet 7606  \\\n",
       "0000        0.0      0.0       0.0  ...         0.0         0.0         0.0   \n",
       "001         0.0      0.0       0.0  ...         0.0         0.0         0.0   \n",
       "0011        0.0      0.0       0.0  ...         0.0         0.0         0.0   \n",
       "001116      0.0      0.0       0.0  ...         0.0         0.0         0.0   \n",
       "0025        0.0      0.0       0.0  ...         0.0         0.0         0.0   \n",
       "005225      0.0      0.0       0.0  ...         0.0         0.0         0.0   \n",
       "007         0.0      0.0       0.0  ...         0.0         0.0         0.0   \n",
       "00c         0.0      0.0       0.0  ...         0.0         0.0         0.0   \n",
       "00kj        0.0      0.0       0.0  ...         0.0         0.0         0.0   \n",
       "01          0.0      0.0       0.0  ...         0.0         0.0         0.0   \n",
       "010156      0.0      0.0       0.0  ...         0.0         0.0         0.0   \n",
       "010217      0.0      0.0       0.0  ...         0.0         0.0         0.0   \n",
       "0104        0.0      0.0       0.0  ...         0.0         0.0         0.0   \n",
       "010401      0.0      0.0       0.0  ...         0.0         0.0         0.0   \n",
       "0106        0.0      0.0       0.0  ...         0.0         0.0         0.0   \n",
       "0111        0.0      0.0       0.0  ...         0.0         0.0         0.0   \n",
       "012         0.0      0.0       0.0  ...         0.0         0.0         0.0   \n",
       "012032      0.0      0.0       0.0  ...         0.0         0.0         0.0   \n",
       "012624      0.0      0.0       0.0  ...         0.0         0.0         0.0   \n",
       "014         0.0      0.0       0.0  ...         0.0         0.0         0.0   \n",
       "\n",
       "        Tweet 7607  Tweet 7608  Tweet 7609  Tweet 7610  Tweet 7611  \\\n",
       "0000           0.0         0.0         0.0         0.0     0.00000   \n",
       "001            0.0         0.0         0.0         0.0     0.00000   \n",
       "0011           0.0         0.0         0.0         0.0     0.00000   \n",
       "001116         0.0         0.0         0.0         0.0     0.00000   \n",
       "0025           0.0         0.0         0.0         0.0     0.00000   \n",
       "005225         0.0         0.0         0.0         0.0     0.00000   \n",
       "007            0.0         0.0         0.0         0.0     0.00000   \n",
       "00c            0.0         0.0         0.0         0.0     0.00000   \n",
       "00kj           0.0         0.0         0.0         0.0     0.00000   \n",
       "01             0.0         0.0         0.0         0.0     0.00000   \n",
       "010156         0.0         0.0         0.0         0.0     0.00000   \n",
       "010217         0.0         0.0         0.0         0.0     0.00000   \n",
       "0104           0.0         0.0         0.0         0.0     8.55145   \n",
       "010401         0.0         0.0         0.0         0.0     0.00000   \n",
       "0106           0.0         0.0         0.0         0.0     0.00000   \n",
       "0111           0.0         0.0         0.0         0.0     0.00000   \n",
       "012            0.0         0.0         0.0         0.0     0.00000   \n",
       "012032         0.0         0.0         0.0         0.0     0.00000   \n",
       "012624         0.0         0.0         0.0         0.0     0.00000   \n",
       "014            0.0         0.0         0.0         0.0     0.00000   \n",
       "\n",
       "        Tweet 7612  Tweet 7613  \n",
       "0000           0.0         0.0  \n",
       "001            0.0         0.0  \n",
       "0011           0.0         0.0  \n",
       "001116         0.0         0.0  \n",
       "0025           0.0         0.0  \n",
       "005225         0.0         0.0  \n",
       "007            0.0         0.0  \n",
       "00c            0.0         0.0  \n",
       "00kj           0.0         0.0  \n",
       "01             0.0         0.0  \n",
       "010156         0.0         0.0  \n",
       "010217         0.0         0.0  \n",
       "0104           0.0         0.0  \n",
       "010401         0.0         0.0  \n",
       "0106           0.0         0.0  \n",
       "0111           0.0         0.0  \n",
       "012            0.0         0.0  \n",
       "012032         0.0         0.0  \n",
       "012624         0.0         0.0  \n",
       "014            0.0         0.0  \n",
       "\n",
       "[20 rows x 7613 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
=======
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         level_0  0000  001  0011  001116  0025  005225  007  00c  00kj  ...  \\\n",
      "0        Tweet 1   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  ...   \n",
      "1        Tweet 2   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  ...   \n",
      "2        Tweet 3   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  ...   \n",
      "3        Tweet 4   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  ...   \n",
      "4        Tweet 5   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  ...   \n",
      "...          ...   ...  ...   ...     ...   ...     ...  ...  ...   ...  ...   \n",
      "7608  Tweet 7609   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  ...   \n",
      "7609  Tweet 7610   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  ...   \n",
      "7610  Tweet 7611   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  ...   \n",
      "7611  Tweet 7612   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  ...   \n",
      "7612  Tweet 7613   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  ...   \n",
      "\n",
      "      zz4  zzaes  zzcb  zzi  zzk  zztb  zzxgh  zzxt  zzzg  zzzz  \n",
      "0     0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "1     0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "2     0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "3     0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "4     0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "...   ...    ...   ...  ...  ...   ...    ...   ...   ...   ...  \n",
      "7608  0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "7609  0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "7610  0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "7611  0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "7612  0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "\n",
      "[7613 rows x 23875 columns]\n"
     ]
>>>>>>> ce2ec3a (This is the newest version of wrapper)
    }
   ],
   "source": [
    "# first attempts at tf-idf\n",
    "# input needs to exclude mentions & hashtags, be in lower case, remove punct, lemmatized\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(norm=None)\n",
    "tfidf_scores = vectorizer.fit_transform(df.processed_tweet)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# get corpus index\n",
    "corpus_index = [f\"Tweet {i+1}\" for i in range(len(df.processed_tweet))]\n",
    "\n",
    "# create pandas DataFrame with tf-idf scores\n",
    "df_tf_idf = pd.DataFrame(tfidf_scores.T.todense(), index=feature_names, columns=corpus_index)\n",
    "# df_tf_idf.reset_index()\n",
    "\n",
    "df_tf_idf = df_tf_idf.T.reset_index()\n",
    "\n",
    "print(df_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'lemmatized_tweet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'lemmatized_tweet'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-cf3e371b0d19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# removes 'stop words' such as 'the', 'are', etc. it knows these stop words where i defined 'stop' variable, comes from a library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet_stop'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lemmatized_tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# hashtags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'lemmatized_tweet'"
     ]
    }
   ],
   "source": [
    "# IGNORE FOR RIGHT NOW\n",
    "# don't think we need to remove stop words with tf-idf but keep here\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "# removes 'stop words' such as 'the', 'are', etc. it knows these stop words where i defined 'stop' variable, comes from a library\n",
    "df['tweet_stop'] = df['lemmatized_tweet'].apply(lambda x: [y for y in x if y not in stop])\n",
    "\n",
    "# hashtags\n",
    "df['hashtag'] = df.text.apply(lambda x: re.findall(r\"#(\\w+)\", x))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'tweet_stop'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0a9a1e93f80b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# IGNORE FOR RIGHT NOW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweet_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweet_stop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweet_stop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5273\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'tweet_stop'"
     ]
    }
   ],
   "source": [
    "# IGNORE FOR RIGHT NOW\n",
    "\n",
    "df.tweet_stop = df.tweet_stop.apply(lambda x: [' '.join(str(y)) for y in x])\n",
    "print(df.tweet_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'tweet_stop'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-17fc314e8676>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweet_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweet_stop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# initialize and fit CountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5273\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'tweet_stop'"
     ]
    }
   ],
   "source": [
    "# IGNORE FOR RIGHT NOW\n",
    "\n",
    "# this is my attempt at creating a matrix of term frequencies using tf-idf but the problem is that\n",
    "# a lot of the 'words' people use are literally jibberish\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df.tweet_stop = df.tweet_stop.apply(lambda x: [''.join(str(y)) for y in x])\n",
    "# initialize and fit CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "term_frequencies = vectorizer.fit_transform(df.text)\n",
    "\n",
    "# get vocabulary of terms\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# get corpus index\n",
    "corpus_index = [f\"Tweet {i+1}\" for i in range(len(df.tokenized_tweet))]\n",
    "\n",
    "# create pandas DataFrame with term frequencies\n",
    "df_term_frequencies = pd.DataFrame(term_frequencies.T.todense(), index=feature_names, columns=corpus_index)\n",
    "\n",
    "# df_term_frequencies.head(30)\n",
    "print(df_term_frequencies.iloc[:])\n",
    "\n",
    "df_term_frequencies['frequency_summation'] = df_term_frequencies.iloc[:].sum(axis=1)\n",
    "print(df_term_frequencies.iloc[:])\n",
    "\n",
    "df_term_frequencies = df_term_frequencies[df_term_frequencies['frequency_summation'] >= 2]\n",
    "# df_term_frequencies = df_term_frequencies.loc[df_term_frequencies.frequency_summation >= 5]\n",
    "print(df_term_frequencies.iloc[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'whatever_our_big_matrix_is_called' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-81a809148431>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Split up the matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhatever_our_big_matrix_is_called\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# feature matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhatever_our_big_matrix_is_called\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m               \u001b[0;31m# target feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwhatever_our_big_matrix_is_called\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'whatever_our_big_matrix_is_called' is not defined"
     ]
    }
   ],
   "source": [
=======
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      target\n",
      "0          1\n",
      "1          1\n",
      "2          1\n",
      "3          1\n",
      "4          1\n",
      "...      ...\n",
      "7608       1\n",
      "7609       1\n",
      "7610       1\n",
      "7611       1\n",
      "7612       1\n",
      "\n",
      "[7613 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# put targets into their own dataframe so i can merge them with tf-idf scores\n",
    "target_df = pd.DataFrame()\n",
    "target_df['target'] = df.target\n",
    "print(target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE FOR RIGHT NOW\n",
    "# don't think we need to remove stop words with tf-idf but keep here\n",
    "\n",
    "#stop = set(stopwords.words('english'))\n",
    "\n",
    "# removes 'stop words' such as 'the', 'are', etc. it knows these stop words where i defined 'stop' variable, comes from a library\n",
    "#df['tweet_stop'] = df['lemmatized_tweet'].apply(lambda x: [y for y in x if y not in stop])\n",
    "\n",
    "# hashtags\n",
    "#df['hashtag'] = df.text.apply(lambda x: re.findall(r\"#(\\w+)\", x))\n",
    "\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE FOR RIGHT NOW\n",
    "\n",
    "#df.tweet_stop = df.tweet_stop.apply(lambda x: [' '.join(str(y)) for y in x])\n",
    "#print(df.tweet_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Tweet 1  Tweet 2  Tweet 3  Tweet 4  Tweet 5  Tweet 6  Tweet 7  \\\n",
      "0000          0        0        0        0        0        0        0   \n",
      "001           0        0        0        0        0        0        0   \n",
      "0011          0        0        0        0        0        0        0   \n",
      "001116        0        0        0        0        0        0        0   \n",
      "0025          0        0        0        0        0        0        0   \n",
      "...         ...      ...      ...      ...      ...      ...      ...   \n",
      "zztb          0        0        0        0        0        0        0   \n",
      "zzxgh         0        0        0        0        0        0        0   \n",
      "zzxt          0        0        0        0        0        0        0   \n",
      "zzzg          0        0        0        0        0        0        0   \n",
      "zzzz          0        0        0        0        0        0        0   \n",
      "\n",
      "        Tweet 8  Tweet 9  Tweet 10  ...  Tweet 7605  Tweet 7606  Tweet 7607  \\\n",
      "0000          0        0         0  ...           0           0           0   \n",
      "001           0        0         0  ...           0           0           0   \n",
      "0011          0        0         0  ...           0           0           0   \n",
      "001116        0        0         0  ...           0           0           0   \n",
      "0025          0        0         0  ...           0           0           0   \n",
      "...         ...      ...       ...  ...         ...         ...         ...   \n",
      "zztb          0        0         0  ...           0           0           0   \n",
      "zzxgh         0        0         0  ...           0           0           0   \n",
      "zzxt          0        0         0  ...           0           0           0   \n",
      "zzzg          0        0         0  ...           0           0           0   \n",
      "zzzz          0        0         0  ...           0           0           0   \n",
      "\n",
      "        Tweet 7608  Tweet 7609  Tweet 7610  Tweet 7611  Tweet 7612  \\\n",
      "0000             0           0           0           0           0   \n",
      "001              0           0           0           0           0   \n",
      "0011             0           0           0           0           0   \n",
      "001116           0           0           0           0           0   \n",
      "0025             0           0           0           0           0   \n",
      "...            ...         ...         ...         ...         ...   \n",
      "zztb             0           0           0           0           0   \n",
      "zzxgh            0           0           0           0           0   \n",
      "zzxt             0           0           0           0           0   \n",
      "zzzg             0           0           0           0           0   \n",
      "zzzz             0           0           0           0           0   \n",
      "\n",
      "        Tweet 7613  frequency_summation  \n",
      "0000             0                    1  \n",
      "001              0                    2  \n",
      "0011             0                    1  \n",
      "001116           0                    2  \n",
      "0025             0                    1  \n",
      "...            ...                  ...  \n",
      "zztb             0                    1  \n",
      "zzxgh            0                    1  \n",
      "zzxt             0                    1  \n",
      "zzzg             0                    1  \n",
      "zzzz             0                    1  \n",
      "\n",
      "[23874 rows x 7614 columns]\n"
     ]
    }
   ],
   "source": [
    "# IGNORE FOR RIGHT NOW - THIS IS THE COUNT VECTORIZER FOR IF WE WANT TO REMOVE KEYBOARD SMASHES LATER\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# remove terms that appear only once\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "term_frequencies = vectorizer.fit_transform(df.processed_tweet)\n",
    "\n",
    "# get vocabulary of terms\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "corpus_index = [f\"Tweet {i+1}\" for i in range(len(df.processed_tweet))]\n",
    "\n",
    "# create pandas DataFrame with term frequencies\n",
    "df_term_frequencies = pd.DataFrame(term_frequencies.T.todense(), index=feature_names, columns=corpus_index)\n",
    "\n",
    "df_term_frequencies['frequency_summation'] = df_term_frequencies.iloc[:].sum(axis=1)\n",
    "# df_term_frequencies = df_term_frequencies[df_term_frequencies['frequency_summation'] >= 2]\n",
    "print(df_term_frequencies.iloc[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0011</th>\n",
       "      <th>001116</th>\n",
       "      <th>0025</th>\n",
       "      <th>005225</th>\n",
       "      <th>007</th>\n",
       "      <th>00c</th>\n",
       "      <th>00kj</th>\n",
       "      <th>01</th>\n",
       "      <th>010156</th>\n",
       "      <th>010217</th>\n",
       "      <th>...</th>\n",
       "      <th>zz4</th>\n",
       "      <th>zzaes</th>\n",
       "      <th>zzcb</th>\n",
       "      <th>zzi</th>\n",
       "      <th>zzk</th>\n",
       "      <th>zztb</th>\n",
       "      <th>zzxgh</th>\n",
       "      <th>zzxt</th>\n",
       "      <th>zzzg</th>\n",
       "      <th>zzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23872 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0011  001116  0025  005225  007  00c  00kj   01  010156  010217  ...  zz4  \\\n",
       "0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0     0.0     0.0  ...  0.0   \n",
       "1   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0     0.0     0.0  ...  0.0   \n",
       "2   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0     0.0     0.0  ...  0.0   \n",
       "3   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0     0.0     0.0  ...  0.0   \n",
       "4   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0     0.0     0.0  ...  0.0   \n",
       "\n",
       "   zzaes  zzcb  zzi  zzk  zztb  zzxgh  zzxt  zzzg  zzzz  \n",
       "0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
       "1    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
       "2    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
       "3    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
       "4    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 23872 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
>>>>>>> ce2ec3a (This is the newest version of wrapper)
    "# Split up the matrix\n",
    "df_tf_idf.drop(df_tf_idf.columns[0], axis=1, inplace=True)\n",
    "features = df_tf_idf    # feature matrix\n",
    "labels = target_df['target']              # target feature\n",
    "df_tf_idf.head()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the features and the labels:\n",
      "\n",
      "train features:\n",
      "      0011  001116  0025  005225  007  00c  00kj   01  010156  010217  ...  \\\n",
      "5151   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0     0.0     0.0  ...   \n",
      "6351   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0     0.0     0.0  ...   \n",
      "3443   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0     0.0     0.0  ...   \n",
      "7164   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0     0.0     0.0  ...   \n",
      "7037   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0     0.0     0.0  ...   \n",
      "...    ...     ...   ...     ...  ...  ...   ...  ...     ...     ...  ...   \n",
      "5226   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0     0.0     0.0  ...   \n",
      "5390   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0     0.0     0.0  ...   \n",
      "860    0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0     0.0     0.0  ...   \n",
      "7603   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0     0.0     0.0  ...   \n",
      "7270   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0     0.0     0.0  ...   \n",
      "\n",
      "      zz4  zzaes  zzcb  zzi  zzk  zztb  zzxgh  zzxt  zzzg  zzzz  \n",
      "5151  0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "6351  0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "3443  0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "7164  0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "7037  0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "...   ...    ...   ...  ...  ...   ...    ...   ...   ...   ...  \n",
      "5226  0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "5390  0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "860   0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "7603  0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "7270  0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "\n",
      "[5709 rows x 23872 columns]\n",
      "test features:\n",
      "      0011  001116  0025  005225  007  00c  00kj   01  010156  010217  ...  \\\n",
      "2644   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0     0.0     0.0  ...   \n",
      "2227   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0     0.0     0.0  ...   \n",
      "5448   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0     0.0     0.0  ...   \n",
      "132    0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0     0.0     0.0  ...   \n",
      "6845   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0     0.0     0.0  ...   \n",
      "...    ...     ...   ...     ...  ...  ...   ...  ...     ...     ...  ...   \n",
      "5209   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0     0.0     0.0  ...   \n",
      "387    0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0     0.0     0.0  ...   \n",
      "4848   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0     0.0     0.0  ...   \n",
      "1032   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0     0.0     0.0  ...   \n",
      "7195   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0     0.0     0.0  ...   \n",
      "\n",
      "      zz4  zzaes  zzcb  zzi  zzk  zztb  zzxgh  zzxt  zzzg  zzzz  \n",
      "2644  0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "2227  0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "5448  0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "132   0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "6845  0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "...   ...    ...   ...  ...  ...   ...    ...   ...   ...   ...  \n",
      "5209  0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "387   0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "4848  0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "1032  0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "7195  0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "\n",
      "[1904 rows x 23872 columns]\n",
      "train labels:\n",
      "5151    0\n",
      "6351    0\n",
      "3443    0\n",
      "7164    1\n",
      "7037    1\n",
      "       ..\n",
      "5226    0\n",
      "5390    0\n",
      "860     0\n",
      "7603    1\n",
      "7270    1\n",
      "Name: target, Length: 5709, dtype: int64\n",
      "test labels:\n",
      "2644    1\n",
      "2227    0\n",
      "5448    1\n",
      "132     0\n",
      "6845    0\n",
      "       ..\n",
      "5209    0\n",
      "387     1\n",
      "4848    1\n",
      "1032    0\n",
      "7195    1\n",
      "Name: target, Length: 1904, dtype: int64\n"
     ]
    }
   ],
>>>>>>> ce2ec3a (This is the newest version of wrapper)
   "source": [
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)\n",
    "\n",
    "print('These are the features and the labels:\\n')\n",
    "print('train features:')\n",
    "print(train_features)\n",
    "print('test features:')\n",
    "print(test_features)\n",
    "print('train labels:')\n",
    "print(train_labels)\n",
    "print('test labels:')\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapper method"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9f3dd8f24121>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best combination (ACC: %.3f): %s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_score_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_feature_idx_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_features' is not defined"
=======
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "STOPPING EARLY DUE TO KEYBOARD INTERRUPT..."
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-88ac0d86a9ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best combination (ACC: %.3f): %s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_score_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_feature_idx_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/mlxtend/feature_selection/sequential_feature_selector.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, custom_feature_names, groups, **fit_params)\u001b[0m\n\u001b[1;32m    566\u001b[0m                     \u001b[0mbest_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0mk_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m             \u001b[0mk_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubsets_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_subset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feature_idx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_features\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'parsimonious'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: None"
>>>>>>> ce2ec3a (This is the newest version of wrapper)
     ]
    }
   ],
   "source": [
    "# Wrapper method 1 with k range - selma\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "sfs = SFS(RandomForestClassifier(), \n",
    "           k_features=(23000, 23002),\n",
    "           forward=True, \n",
    "           floating=False, \n",
    "           scoring='accuracy',\n",
    "           cv=5)\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), sfs)\n",
    "\n",
    "pipe.fit(train_features, train_labels)\n",
    "\n",
    "print('best combination (ACC: %.3f): %s\\n' % (sfs.k_score_, sfs.k_feature_idx_))\n",
    "sfs.k_feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper method 2 - selma\n",
    "sfs = SFS(RandomForestClassifier(),\n",
    "          k_features=2828,\n",
    "          forward=True,\n",
    "          floating=False,\n",
    "          scoring = 'accuracy',\n",
    "          cv = 0)\n",
    "\n",
    "sfs.fit(train_features, train_labels)\n",
    "\n",
    "#print('best combination (ACC: %.3f): %s\\n' % (sfs.k_score_, sfs.k_feature_idx_))\n",
    "print('best combination (ACC: %.3f)' % (sfs.k_score_))\n",
    "#sfs.k_feature_names_     # to get the final set of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter method"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4e183376987b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#I think tweets must be preprocessed first though\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mconstant_features_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mconstant_features_testing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_features' is not defined"
=======
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
>>>>>>> ce2ec3a (This is the newest version of wrapper)
     ]
    }
   ],
   "source": [
    "#Remove the constants, does not run\n",
    "#I think tweets must be preprocessed first though\n",
<<<<<<< HEAD
    "\n",
    "constant_features_training = [var for var in train_features.columns if X_train[var].std() == 0] \n",
    "\n",
    "train_features.drop(labels=constant_features_training, axis=1, inplace=True)\n",
    "\n",
    "train_features.shape"
=======
    "constant_features = [var for var in train_features.columns if train_features[var].std() == 0] \n",
    "\n",
    "no_constant_train = train_features.drop(labels=constant_features, axis=1, inplace=True)\n",
    "no_constant_test = test_features.drop(labels=constant_features, axis=1, inplace=True) \n",
    " \n",
    "\n",
    "print(no_constant_train)"
>>>>>>> ce2ec3a (This is the newest version of wrapper)
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'forest%20fires'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-1882c77feb21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mq_remover\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVarianceThreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Find the values with low variance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mq_remover\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_remover\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Apply to datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/feature_selection/_variance_threshold.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mitself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \"\"\"\n\u001b[0;32m---> 91\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    744\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m                 raise ValueError(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1992\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNpDtype\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1993\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1995\u001b[0m     def __array_wrap__(\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'forest%20fires'"
     ]
    }
   ],
   "source": [
    "#Removing Quasi-constant features,does not run\n",
    "\n",
    "# Define the threshold as 0.01, check value for variance treshold\n",
    "quasi_remover = VarianceThreshold(threshold=0.01)\n",
    "# Find the values with low variance\n",
<<<<<<< HEAD
    "quasi_remover.fit(train_features) \n",
    "sum(quasi_remover.get_support())\n",
    "# Apply to datasets\n",
    "train_features = quasi_remover.transform(train_features)\n",
    "\n",
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignore this\n",
    "#X_train= pd.DataFrame(X_train)\n",
    "#X_train.columns = features\n",
    "\n",
    "#X_test= pd.DataFrame(X_test)\n",
    "#X_test.columns = features"
=======
    "quasi_remover.fit(no_constant_train) \n",
    "sum(quasi_remover.get_support())\n",
    "# Apply to datasets\n",
    "no_quasi_train = quasi_remover.transform(no_constant_train)\n",
    "no_quasi_test = quasi_remover.transform(no_constant_test)\n",
    "\n",
    "no_quasi_train.shape, no_quasi_test.shape"
>>>>>>> ce2ec3a (This is the newest version of wrapper)
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVE DUPLICATE FEATURES\n",
    "duplFeatures = []\n",
    "for i in range(0, len(train_features.columns)):\n",
    "    oneCol = train_features.columns[i]\n",
    "for othCol in train_features.columns[i + 1:]:\n",
    "    if train_features[oneCol].equals(train_features[othCol]):\n",
    "            duplFeatures.append(othCol)\n",
    "            \n",
    "train_features.drop(labels=duplFeatures, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-13-a883aba494a2>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-a883aba494a2>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    colName = correl_matrix.columns[i]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "#REMOVE CORRELATED FEATURES\n",
    "correl_Feat = set() \n",
    "correl_matrix = dataset.corr()\n",
    "    \n",
    "for i in range(len(corr_matrix.columns)):\n",
    "   for j in range(i):\n",
    "       if abs(correl_matrix.iloc[i, j]) > 0.8:\n",
    "           colName = correl_matrix.columns[i]  \n",
    "           correl_Feat.add(colname)\n",
    "        \n",
    "X_train.drop(labels=correl_Feat, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after each filter or removing of features,\n",
    "#a copy can be made and stored to later compare performance. We can check performance of original list,\n",
    "#vs after removing correlated vs after removing quasi constants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR WRAPPER METHOD - GENERATE THE NEW TRAIN AND TEST DATAFRAMES BASED ON SELECTED FEATURES\n",
    "\n",
    "# Note that the transform call is equivalent to\n",
    "# features_train[:, sfs.k_feature_idx_]\n",
    "\n",
    "features_train_sfs = sfs.transform(X_train)\n",
    "features_test_sfs = sfs.transform(X_test)\n",
    "print(features_train_sfs)\n",
    "print(features_test_sfs)\n",
    "\n",
    "# Fit the estimator using the new feature subset\n",
    "# and make a prediction on the test data\n",
    "model = RandomForestClassifier()\n",
    "model.fit(features_train_sfs, labels_train)\n",
    "labels_pred = model.predict(features_test_sfs)\n",
    "\n",
    "# Compute the accuracy of the prediction\n",
    "acc = float((labels_test == labels_pred).sum()) / labels_pred.shape[0]\n",
    "print('Test set accuracy: %.2f %%' % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest - train the model and test, report score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "print('\\nThis is RandomForest score:')\n",
    "model = RandomForestClassifier()\n",
    "model.fit(train_features, train_labels)\n",
    "print(model.score(test_features, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression - train the model and test, report score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "print('\\nThis is Linear Regression score:')\n",
    "model2 = LinearRegression()\n",
    "model2.fit(train_features, train_labels)\n",
    "model2.score(test_features, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
