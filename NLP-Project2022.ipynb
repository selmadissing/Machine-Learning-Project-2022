{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Machine Learning Project 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links:\n",
    "\n",
    "https://towardsdatascience.com/natural-language-processing-nlp-for-machine-learning-d44498845d5b\n",
    "https://www.andyfitzgeraldconsulting.com/writing/keyword-extraction-nlp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import re\n",
    "# from term_frequency import term_frequencies, feature_names, df_term_frequencies\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "transformer = TfidfTransformer()\n",
    "tt = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN this only for the first time and then comment:\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id          61\n",
      "keyword      0\n",
      "location     0\n",
      "text        61\n",
      "target      61\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'train.csv')\n",
    "\n",
    "df.replace('NaN', np.NaN, inplace = True)\n",
    "\n",
    "# to count the number of NaN's in each column, just change the column name in this line to see how many missing values of that\n",
    "# variable per other column\n",
    "print(df[df.keyword.isnull()].count())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Removing punctuation\n",
    "import string\n",
    "\n",
    "def remove_punct(text):\n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "\n",
    "# df[\"text_clean\"] = df[\"text\"].apply(lambda x: remove_punct(x))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "# divides tweet into list of its words\n",
    "df['tokenized_tweet'] = df.apply(lambda row: tt.tokenize(row['text']), axis=1)\n",
    "\n",
    "# 'stemmer' reduces all words to their stems by bluntly cutting off prefixes - not useful with names, etc.\n",
    "df['stemmed_tweet'] = df['tokenized_tweet'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "\n",
    "# 'lemmatizer' gets rid of plurals, etc., it's gentler than stemming\n",
    "df['lemmatized_tweet'] = df['tokenized_tweet'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x]) \n",
    "\n",
    "# removes 'stop words' such as 'the', 'are', etc. it knows these stop words where i defined 'stop' variable, comes from a library\n",
    "df['tweet_stop'] = df['lemmatized_tweet'].apply(lambda x: [y for y in x if y not in stop])\n",
    "\n",
    "# hashtags\n",
    "df['hashtag'] = df.text.apply(lambda x: re.findall(r\"#(\\w+)\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tokenized_tweet</th>\n",
       "      <th>stemmed_tweet</th>\n",
       "      <th>lemmatized_tweet</th>\n",
       "      <th>tweet_stop</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Our, Deeds, are, the, Reason, of, this, #eart...</td>\n",
       "      <td>[our, deed, are, the, reason, of, thi, #earthq...</td>\n",
       "      <td>[Our, Deeds, are, the, Reason, of, this, #eart...</td>\n",
       "      <td>[Our, Deeds, Reason, #earthquake, May, ALLAH, ...</td>\n",
       "      <td>[earthquake]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>[Forest, fire, near, La, Ronge, Sask, ., Canada]</td>\n",
       "      <td>[forest, fire, near, la, rong, sask, ., canada]</td>\n",
       "      <td>[Forest, fire, near, La, Ronge, Sask, ., Canada]</td>\n",
       "      <td>[Forest, fire, near, La, Ronge, Sask, ., Canada]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[All, residents, asked, to, ', shelter, in, pl...</td>\n",
       "      <td>[all, resid, ask, to, ', shelter, in, place, '...</td>\n",
       "      <td>[All, resident, asked, to, ', shelter, in, pla...</td>\n",
       "      <td>[All, resident, asked, ', shelter, place, ', n...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>[13,000, people, receive, #wildfires, evacuati...</td>\n",
       "      <td>[13,000, peopl, receiv, #wildfir, evacu, order...</td>\n",
       "      <td>[13,000, people, receive, #wildfires, evacuati...</td>\n",
       "      <td>[13,000, people, receive, #wildfires, evacuati...</td>\n",
       "      <td>[wildfires]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Just, got, sent, this, photo, from, Ruby, #Al...</td>\n",
       "      <td>[just, got, sent, thi, photo, from, rubi, #ala...</td>\n",
       "      <td>[Just, got, sent, this, photo, from, Ruby, #Al...</td>\n",
       "      <td>[Just, got, sent, photo, Ruby, #Alaska, smoke,...</td>\n",
       "      <td>[Alaska, wildfires]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                    tokenized_tweet  \\\n",
       "0       1  [Our, Deeds, are, the, Reason, of, this, #eart...   \n",
       "1       1   [Forest, fire, near, La, Ronge, Sask, ., Canada]   \n",
       "2       1  [All, residents, asked, to, ', shelter, in, pl...   \n",
       "3       1  [13,000, people, receive, #wildfires, evacuati...   \n",
       "4       1  [Just, got, sent, this, photo, from, Ruby, #Al...   \n",
       "\n",
       "                                       stemmed_tweet  \\\n",
       "0  [our, deed, are, the, reason, of, thi, #earthq...   \n",
       "1    [forest, fire, near, la, rong, sask, ., canada]   \n",
       "2  [all, resid, ask, to, ', shelter, in, place, '...   \n",
       "3  [13,000, peopl, receiv, #wildfir, evacu, order...   \n",
       "4  [just, got, sent, thi, photo, from, rubi, #ala...   \n",
       "\n",
       "                                    lemmatized_tweet  \\\n",
       "0  [Our, Deeds, are, the, Reason, of, this, #eart...   \n",
       "1   [Forest, fire, near, La, Ronge, Sask, ., Canada]   \n",
       "2  [All, resident, asked, to, ', shelter, in, pla...   \n",
       "3  [13,000, people, receive, #wildfires, evacuati...   \n",
       "4  [Just, got, sent, this, photo, from, Ruby, #Al...   \n",
       "\n",
       "                                          tweet_stop              hashtag  \n",
       "0  [Our, Deeds, Reason, #earthquake, May, ALLAH, ...         [earthquake]  \n",
       "1   [Forest, fire, near, La, Ronge, Sask, ., Canada]                   []  \n",
       "2  [All, resident, asked, ', shelter, place, ', n...                   []  \n",
       "3  [13,000, people, receive, #wildfires, evacuati...          [wildfires]  \n",
       "4  [Just, got, sent, photo, Ruby, #Alaska, smoke,...  [Alaska, wildfires]  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_part_of_speech():\n",
    "    POS = wordnet.synsets(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [O u r, D e e d s, R e a s o n, # e a r t h q ...\n",
      "1       [F o r e s t, f i r e, n e a r, L a, R o n g e...\n",
      "2       [A l l, r e s i d e n t, a s k e d, ', s h e l...\n",
      "3       [1 3 , 0 0 0, p e o p l e, r e c e i v e, # w ...\n",
      "4       [J u s t, g o t, s e n t, p h o t o, R u b y, ...\n",
      "                              ...                        \n",
      "7608    [T w o, g i a n t, c r a n e, h o l d i n g, b...\n",
      "7609    [@ a r i a _ a h r a r y, @ T h e T a w n i e ...\n",
      "7610    [M 1, ., 9 4, [, 0 1 : 0 4, U T C, ], ?, 5 k m...\n",
      "7611    [P o l i c e, i n v e s t i g a t i n g, e - b...\n",
      "7612    [T h e, L a t e s t, :, M o r e, H o m e s, R ...\n",
      "Name: tweet_stop, Length: 7613, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df.tweet_stop = df.tweet_stop.apply(lambda x: [' '.join(str(y)) for y in x])\n",
    "print(df.tweet_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Tweet 1  Tweet 2  Tweet 3  Tweet 4  Tweet 5  Tweet 6  Tweet 7  \\\n",
      "00                  0        0        0        0        0        0        0   \n",
      "000                 0        0        0        1        0        0        0   \n",
      "0000                0        0        0        0        0        0        0   \n",
      "007npen6lg          0        0        0        0        0        0        0   \n",
      "00cy9vxeff          0        0        0        0        0        0        0   \n",
      "...               ...      ...      ...      ...      ...      ...      ...   \n",
      "ûóher               0        0        0        0        0        0        0   \n",
      "ûókody              0        0        0        0        0        0        0   \n",
      "ûónegligence        0        0        0        0        0        0        0   \n",
      "ûótech              0        0        0        0        0        0        0   \n",
      "ûówe                0        0        0        0        0        0        0   \n",
      "\n",
      "              Tweet 8  Tweet 9  Tweet 10  ...  Tweet 7604  Tweet 7605  \\\n",
      "00                  0        0         0  ...           0           0   \n",
      "000                 0        0         0  ...           0           0   \n",
      "0000                0        0         0  ...           0           0   \n",
      "007npen6lg          0        0         0  ...           0           0   \n",
      "00cy9vxeff          0        0         0  ...           0           0   \n",
      "...               ...      ...       ...  ...         ...         ...   \n",
      "ûóher               0        0         0  ...           0           0   \n",
      "ûókody              0        0         0  ...           0           0   \n",
      "ûónegligence        0        0         0  ...           0           0   \n",
      "ûótech              0        0         0  ...           0           0   \n",
      "ûówe                0        0         0  ...           0           0   \n",
      "\n",
      "              Tweet 7606  Tweet 7607  Tweet 7608  Tweet 7609  Tweet 7610  \\\n",
      "00                     0           0           0           0           0   \n",
      "000                    0           0           0           0           0   \n",
      "0000                   0           0           0           0           0   \n",
      "007npen6lg             0           0           0           0           0   \n",
      "00cy9vxeff             0           0           0           0           0   \n",
      "...                  ...         ...         ...         ...         ...   \n",
      "ûóher                  0           0           0           0           0   \n",
      "ûókody                 0           0           0           0           0   \n",
      "ûónegligence           0           0           0           0           0   \n",
      "ûótech                 0           0           0           0           0   \n",
      "ûówe                   0           0           0           0           0   \n",
      "\n",
      "              Tweet 7611  Tweet 7612  Tweet 7613  \n",
      "00                     0           0           0  \n",
      "000                    0           0           0  \n",
      "0000                   0           0           0  \n",
      "007npen6lg             0           0           0  \n",
      "00cy9vxeff             0           0           0  \n",
      "...                  ...         ...         ...  \n",
      "ûóher                  0           0           0  \n",
      "ûókody                 0           0           0  \n",
      "ûónegligence           0           0           0  \n",
      "ûótech                 0           0           0  \n",
      "ûówe                   0           0           0  \n",
      "\n",
      "[21637 rows x 7613 columns]\n",
      "              Tweet 1  Tweet 2  Tweet 3  Tweet 4  Tweet 5  Tweet 6  Tweet 7  \\\n",
      "00                  0        0        0        0        0        0        0   \n",
      "000                 0        0        0        1        0        0        0   \n",
      "0000                0        0        0        0        0        0        0   \n",
      "007npen6lg          0        0        0        0        0        0        0   \n",
      "00cy9vxeff          0        0        0        0        0        0        0   \n",
      "...               ...      ...      ...      ...      ...      ...      ...   \n",
      "ûóher               0        0        0        0        0        0        0   \n",
      "ûókody              0        0        0        0        0        0        0   \n",
      "ûónegligence        0        0        0        0        0        0        0   \n",
      "ûótech              0        0        0        0        0        0        0   \n",
      "ûówe                0        0        0        0        0        0        0   \n",
      "\n",
      "              Tweet 8  Tweet 9  Tweet 10  ...  Tweet 7605  Tweet 7606  \\\n",
      "00                  0        0         0  ...           0           0   \n",
      "000                 0        0         0  ...           0           0   \n",
      "0000                0        0         0  ...           0           0   \n",
      "007npen6lg          0        0         0  ...           0           0   \n",
      "00cy9vxeff          0        0         0  ...           0           0   \n",
      "...               ...      ...       ...  ...         ...         ...   \n",
      "ûóher               0        0         0  ...           0           0   \n",
      "ûókody              0        0         0  ...           0           0   \n",
      "ûónegligence        0        0         0  ...           0           0   \n",
      "ûótech              0        0         0  ...           0           0   \n",
      "ûówe                0        0         0  ...           0           0   \n",
      "\n",
      "              Tweet 7607  Tweet 7608  Tweet 7609  Tweet 7610  Tweet 7611  \\\n",
      "00                     0           0           0           0           0   \n",
      "000                    0           0           0           0           0   \n",
      "0000                   0           0           0           0           0   \n",
      "007npen6lg             0           0           0           0           0   \n",
      "00cy9vxeff             0           0           0           0           0   \n",
      "...                  ...         ...         ...         ...         ...   \n",
      "ûóher                  0           0           0           0           0   \n",
      "ûókody                 0           0           0           0           0   \n",
      "ûónegligence           0           0           0           0           0   \n",
      "ûótech                 0           0           0           0           0   \n",
      "ûówe                   0           0           0           0           0   \n",
      "\n",
      "              Tweet 7612  Tweet 7613  frequency_summation  \n",
      "00                     0           0                   33  \n",
      "000                    0           0                    4  \n",
      "0000                   0           0                    1  \n",
      "007npen6lg             0           0                    1  \n",
      "00cy9vxeff             0           0                    1  \n",
      "...                  ...         ...                  ...  \n",
      "ûóher                  0           0                    1  \n",
      "ûókody                 0           0                    1  \n",
      "ûónegligence           0           0                    1  \n",
      "ûótech                 0           0                    1  \n",
      "ûówe                   0           0                    1  \n",
      "\n",
      "[21637 rows x 7614 columns]\n",
      "        Tweet 1  Tweet 2  Tweet 3  Tweet 4  Tweet 5  Tweet 6  Tweet 7  \\\n",
      "00            0        0        0        0        0        0        0   \n",
      "000           0        0        0        1        0        0        0   \n",
      "01            0        0        0        0        0        0        0   \n",
      "02            0        0        0        0        0        0        0   \n",
      "03            0        0        0        0        0        0        0   \n",
      "...         ...      ...      ...      ...      ...      ...      ...   \n",
      "ûïwe          0        0        0        0        0        0        0   \n",
      "ûïwhen        0        0        0        0        0        0        0   \n",
      "ûïyou         0        0        0        0        0        0        0   \n",
      "ûò            0        0        0        0        0        0        0   \n",
      "ûó            0        0        0        0        0        0        0   \n",
      "\n",
      "        Tweet 8  Tweet 9  Tweet 10  ...  Tweet 7605  Tweet 7606  Tweet 7607  \\\n",
      "00            0        0         0  ...           0           0           0   \n",
      "000           0        0         0  ...           0           0           0   \n",
      "01            0        0         0  ...           0           0           0   \n",
      "02            0        0         0  ...           0           0           0   \n",
      "03            0        0         0  ...           0           0           0   \n",
      "...         ...      ...       ...  ...         ...         ...         ...   \n",
      "ûïwe          0        0         0  ...           0           0           0   \n",
      "ûïwhen        0        0         0  ...           0           0           0   \n",
      "ûïyou         0        0         0  ...           0           0           0   \n",
      "ûò            0        0         0  ...           0           0           0   \n",
      "ûó            0        0         0  ...           0           0           0   \n",
      "\n",
      "        Tweet 7608  Tweet 7609  Tweet 7610  Tweet 7611  Tweet 7612  \\\n",
      "00               0           0           0           0           0   \n",
      "000              0           0           0           0           0   \n",
      "01               0           0           0           1           0   \n",
      "02               0           0           0           0           0   \n",
      "03               0           0           0           0           0   \n",
      "...            ...         ...         ...         ...         ...   \n",
      "ûïwe             0           0           0           0           0   \n",
      "ûïwhen           0           0           0           0           0   \n",
      "ûïyou            0           0           0           0           0   \n",
      "ûò               0           0           0           0           0   \n",
      "ûó               0           0           0           0           0   \n",
      "\n",
      "        Tweet 7613  frequency_summation  \n",
      "00               0                   33  \n",
      "000              0                    4  \n",
      "01               0                   26  \n",
      "02               0                    7  \n",
      "03               0                    5  \n",
      "...            ...                  ...  \n",
      "ûïwe             0                    4  \n",
      "ûïwhen           0                   12  \n",
      "ûïyou            0                    2  \n",
      "ûò               0                   42  \n",
      "ûó               0                   28  \n",
      "\n",
      "[6728 rows x 7614 columns]\n"
     ]
    }
   ],
   "source": [
    "# this is my attempt at creating a matrix of term frequencies using tf-idf but the problem is that\n",
    "# a lot of the 'words' people use are literally jibberish\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df.tweet_stop = df.tweet_stop.apply(lambda x: [''.join(str(y)) for y in x])\n",
    "# initialize and fit CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "term_frequencies = vectorizer.fit_transform(df.text)\n",
    "\n",
    "# get vocabulary of terms\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# get corpus index\n",
    "corpus_index = [f\"Tweet {i+1}\" for i in range(len(df.tokenized_tweet))]\n",
    "\n",
    "# create pandas DataFrame with term frequencies\n",
    "df_term_frequencies = pd.DataFrame(term_frequencies.T.todense(), index=feature_names, columns=corpus_index)\n",
    "\n",
    "# df_term_frequencies.head(30)\n",
    "print(df_term_frequencies.iloc[:])\n",
    "\n",
    "df_term_frequencies['frequency_summation'] = df_term_frequencies.iloc[:].sum(axis=1)\n",
    "print(df_term_frequencies.iloc[:])\n",
    "\n",
    "df_term_frequencies = df_term_frequencies[df_term_frequencies['frequency_summation'] >= 2]\n",
    "# df_term_frequencies = df_term_frequencies.loc[df_term_frequencies.frequency_summation >= 5]\n",
    "print(df_term_frequencies.iloc[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN THIS\n",
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = \n",
    "        train_test_split(features, labels, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper method - selma DO NOT RUN THIS\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "sfs = SFS(RandomForestClassifier(n_jobs=-1), \n",
    "           k_features=(3, 15),\n",
    "           forward=True, \n",
    "           floating=False, \n",
    "           scoring='roc-auc',\n",
    "           cv=5)\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), sfs)\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "print('best combination (ACC: %.3f): %s\\n' % (sfs.k_score_, sfs.k_feature_idx_))\n",
    "print('all subsets:\\n', sfs.subsets_)\n",
    "plot_sfs(sfs.get_metric_dict(), kind='std_err');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
