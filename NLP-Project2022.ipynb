{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Machine Learning Project 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links:\n",
    "\n",
    "https://towardsdatascience.com/natural-language-processing-nlp-for-machine-learning-d44498845d5b\n",
    "https://www.andyfitzgeraldconsulting.com/writing/keyword-extraction-nlp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import re\n",
    "from nltk.probability import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "# from term_frequency import term_frequencies, feature_names, df_term_frequencies\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "transformer = TfidfTransformer()\n",
    "tt = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'train.csv')\n",
    "\n",
    "df.replace('NaN', np.NaN, inplace = True)\n",
    "\n",
    "# to count the number of NaN's in each column, just change the column name in this line to see how many missing values of that\n",
    "# variable per other column\n",
    "# print(df[df.keyword.isnull()].count())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tweet_noNoise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                      tweet_noNoise  \n",
       "0       1  our deeds are the reason of this earthquake ma...  \n",
       "1       1              forest fire near la ronge sask canada  \n",
       "2       1  all residents asked to shelter in place are be...  \n",
       "3       1  13000 people receive wildfires evacuation orde...  \n",
       "4       1  just got sent this photo from ruby alaska as s...  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_tweets(text):\n",
    "    \n",
    "# remove mentions and URLs\n",
    "    text_noMentionURL = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", text)\n",
    "    text_noMentionURL = \" \".join(text_noMentionURL.split())\n",
    "    \n",
    "# remove '#' symbols and add space before capital letter\n",
    "    text_noHash = re.sub(r\"([A-Z]+)\", r\" \\1\", text)\n",
    "    text_noHash = re.sub(r\"(#)\", \"\", text_noHash)\n",
    "    text_noHash = \" \".join(text_noHash.split())\n",
    "    \n",
    "# remove all other punctuation\n",
    "    text_noNoise = \"\".join([char for char in text_noHash if char not in string.punctuation])\n",
    "\n",
    "    return text_noNoise.lower()\n",
    "\n",
    "\n",
    "df['tweet_noNoise'] = df[\"text\"].apply(lambda x: preprocess_tweets(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tweet_noNoise</th>\n",
       "      <th>lemmatized_list</th>\n",
       "      <th>processed_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>[deed, reason, earthquake, may, allah, forgive...</td>\n",
       "      <td>deed,reason,earthquake,may,allah,forgive,u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>forest,fire,near,la,ronge,sask,canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>[resident, ask, shelter, place, notify, office...</td>\n",
       "      <td>resident,ask,shelter,place,notify,officer,evac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>[13000, people, receive, wildfire, evacuation,...</td>\n",
       "      <td>13000,people,receive,wildfire,evacuation,order...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>[get, send, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "      <td>get,send,photo,ruby,alaska,smoke,wildfire,pour...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                      tweet_noNoise  \\\n",
       "0       1  our deeds are the reason of this earthquake ma...   \n",
       "1       1              forest fire near la ronge sask canada   \n",
       "2       1  all residents asked to shelter in place are be...   \n",
       "3       1  13000 people receive wildfires evacuation orde...   \n",
       "4       1  just got sent this photo from ruby alaska as s...   \n",
       "\n",
       "                                     lemmatized_list  \\\n",
       "0  [deed, reason, earthquake, may, allah, forgive...   \n",
       "1      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2  [resident, ask, shelter, place, notify, office...   \n",
       "3  [13000, people, receive, wildfire, evacuation,...   \n",
       "4  [get, send, photo, ruby, alaska, smoke, wildfi...   \n",
       "\n",
       "                                     processed_tweet  \n",
       "0         deed,reason,earthquake,may,allah,forgive,u  \n",
       "1              forest,fire,near,la,ronge,sask,canada  \n",
       "2  resident,ask,shelter,place,notify,officer,evac...  \n",
       "3  13000,people,receive,wildfire,evacuation,order...  \n",
       "4  get,send,photo,ruby,alaska,smoke,wildfire,pour...  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatizer with Part-of-Speech (POS)\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "\n",
    "# this function can be called within lemmatize() to tag the word with its POS\n",
    "def get_pos(word):\n",
    "    probable_part_of_speech = wordnet.synsets(word)\n",
    "  \n",
    "    pos_counts = Counter()\n",
    "\n",
    "    pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "    pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "    pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "    pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "  \n",
    "    most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "    return most_likely_part_of_speech\n",
    "\n",
    "# tokenize tweets = divide into words\n",
    "tokenized_tweets = df.apply(lambda row: tt.tokenize(row['tweet_noNoise']), axis=1)\n",
    "\n",
    "# lemmatize the list of all tokens\n",
    "df['lemmatized_list'] = tokenized_tweets.apply(lambda y: [lemmatizer.lemmatize(x, get_pos(x)) for x in y])\n",
    "df['lemmatized_list'].apply(lambda x: ' '.join([tweet for tweet in x]))\n",
    "\n",
    "# removes stop words (are, on, in, etc.) MUST COME BEFORE LIST -> STR\n",
    "stop = set(stopwords.words('english'))\n",
    "df['lemmatized_list'] = df['lemmatized_list'].apply(lambda x: [y for y in x if y not in stop])\n",
    "\n",
    "# turn the resulting list back into a string\n",
    "df['processed_tweet'] = df.lemmatized_list.agg(lambda x: ','.join(map(str, x)))\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deed\n"
     ]
    }
   ],
   "source": [
    "# enter whatever word you'd like to see how lemmatizing works\n",
    "print(lemmatizer.lemmatize('deeds'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Tweet 1  Tweet 2  Tweet 3  Tweet 4  Tweet 5  Tweet 6  Tweet 7  \\\n",
      "0000        0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "001         0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "0011        0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "001116      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "0025        0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "...         ...      ...      ...      ...      ...      ...      ...   \n",
      "ûó          0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "ûóher       0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "ûókody      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "ûótech      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "ûówe        0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "\n",
      "        Tweet 8  Tweet 9  Tweet 10  ...  Tweet 7604  Tweet 7605  Tweet 7606  \\\n",
      "0000        0.0      0.0       0.0  ...         0.0         0.0         0.0   \n",
      "001         0.0      0.0       0.0  ...         0.0         0.0         0.0   \n",
      "0011        0.0      0.0       0.0  ...         0.0         0.0         0.0   \n",
      "001116      0.0      0.0       0.0  ...         0.0         0.0         0.0   \n",
      "0025        0.0      0.0       0.0  ...         0.0         0.0         0.0   \n",
      "...         ...      ...       ...  ...         ...         ...         ...   \n",
      "ûó          0.0      0.0       0.0  ...         0.0         0.0         0.0   \n",
      "ûóher       0.0      0.0       0.0  ...         0.0         0.0         0.0   \n",
      "ûókody      0.0      0.0       0.0  ...         0.0         0.0         0.0   \n",
      "ûótech      0.0      0.0       0.0  ...         0.0         0.0         0.0   \n",
      "ûówe        0.0      0.0       0.0  ...         0.0         0.0         0.0   \n",
      "\n",
      "        Tweet 7607  Tweet 7608  Tweet 7609  Tweet 7610  Tweet 7611  \\\n",
      "0000           0.0         0.0         0.0         0.0         0.0   \n",
      "001            0.0         0.0         0.0         0.0         0.0   \n",
      "0011           0.0         0.0         0.0         0.0         0.0   \n",
      "001116         0.0         0.0         0.0         0.0         0.0   \n",
      "0025           0.0         0.0         0.0         0.0         0.0   \n",
      "...            ...         ...         ...         ...         ...   \n",
      "ûó             0.0         0.0         0.0         0.0         0.0   \n",
      "ûóher          0.0         0.0         0.0         0.0         0.0   \n",
      "ûókody         0.0         0.0         0.0         0.0         0.0   \n",
      "ûótech         0.0         0.0         0.0         0.0         0.0   \n",
      "ûówe           0.0         0.0         0.0         0.0         0.0   \n",
      "\n",
      "        Tweet 7612  Tweet 7613  \n",
      "0000           0.0         0.0  \n",
      "001            0.0         0.0  \n",
      "0011           0.0         0.0  \n",
      "001116         0.0         0.0  \n",
      "0025           0.0         0.0  \n",
      "...            ...         ...  \n",
      "ûó             0.0         0.0  \n",
      "ûóher          0.0         0.0  \n",
      "ûókody         0.0         0.0  \n",
      "ûótech         0.0         0.0  \n",
      "ûówe           0.0         0.0  \n",
      "\n",
      "[23931 rows x 7613 columns]\n"
     ]
    }
   ],
   "source": [
    "# first attempts at tf-idf\n",
    "# input needs to exclude mentions & hashtags, be in lower case, remove punct, lemmatized\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(norm=None)\n",
    "tfidf_scores = vectorizer.fit_transform(df.processed_tweet)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# get corpus index\n",
    "corpus_index = [f\"Tweet {i+1}\" for i in range(len(df.processed_tweet))]\n",
    "\n",
    "# create pandas DataFrame with tf-idf scores\n",
    "# try:\n",
    "df_tf_idf = pd.DataFrame(tfidf_scores.T.todense(), index=feature_names, columns=corpus_index)\n",
    "print(df_tf_idf)\n",
    "# except:\n",
    "    # pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE FOR RIGHT NOW\n",
    "# don't think we need to remove stop words with tf-idf but keep here\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "# removes 'stop words' such as 'the', 'are', etc. it knows these stop words where i defined 'stop' variable, comes from a library\n",
    "df['tweet_stop'] = df['lemmatized_tweet'].apply(lambda x: [y for y in x if y not in stop])\n",
    "\n",
    "# hashtags\n",
    "df['hashtag'] = df.text.apply(lambda x: re.findall(r\"#(\\w+)\", x))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE FOR RIGHT NOW\n",
    "\n",
    "df.tweet_stop = df.tweet_stop.apply(lambda x: [' '.join(str(y)) for y in x])\n",
    "print(df.tweet_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE FOR RIGHT NOW\n",
    "\n",
    "# this is my attempt at creating a matrix of term frequencies using tf-idf but the problem is that\n",
    "# a lot of the 'words' people use are literally jibberish\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df.tweet_stop = df.tweet_stop.apply(lambda x: [''.join(str(y)) for y in x])\n",
    "# initialize and fit CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "term_frequencies = vectorizer.fit_transform(df.text)\n",
    "\n",
    "# get vocabulary of terms\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# get corpus index\n",
    "corpus_index = [f\"Tweet {i+1}\" for i in range(len(df.tokenized_tweet))]\n",
    "\n",
    "# create pandas DataFrame with term frequencies\n",
    "df_term_frequencies = pd.DataFrame(term_frequencies.T.todense(), index=feature_names, columns=corpus_index)\n",
    "\n",
    "# df_term_frequencies.head(30)\n",
    "print(df_term_frequencies.iloc[:])\n",
    "\n",
    "df_term_frequencies['frequency_summation'] = df_term_frequencies.iloc[:].sum(axis=1)\n",
    "print(df_term_frequencies.iloc[:])\n",
    "\n",
    "df_term_frequencies = df_term_frequencies[df_term_frequencies['frequency_summation'] >= 2]\n",
    "# df_term_frequencies = df_term_frequencies.loc[df_term_frequencies.frequency_summation >= 5]\n",
    "print(df_term_frequencies.iloc[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
