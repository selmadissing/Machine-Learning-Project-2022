{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Machine Learning Project 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links:\n",
    "\n",
    "https://towardsdatascience.com/natural-language-processing-nlp-for-machine-learning-d44498845d5b\n",
    "https://www.andyfitzgeraldconsulting.com/writing/keyword-extraction-nlp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import re\n",
    "from nltk.probability import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "# from term_frequency import term_frequencies, feature_names, df_term_frequencies\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "transformer = TfidfTransformer()\n",
    "tt = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'train.csv')\n",
    "\n",
    "df.replace('NaN', np.NaN, inplace = True)\n",
    "\n",
    "# to count the number of NaN's in each column, just change the column name in this line to see how many missing values of that\n",
    "# variable per other column\n",
    "# print(df[df.keyword.isnull()].count())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tweet_noNoise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfires evacuation orders in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                      tweet_noNoise  \n",
       "0       1  our deeds are the reason of this earthquake ma...  \n",
       "1       1              forest fire near la ronge sask canada  \n",
       "2       1  all residents asked to shelter in place are be...  \n",
       "3       1  people receive wildfires evacuation orders in ...  \n",
       "4       1  just got sent this photo from ruby alaska as s...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_tweets(text):\n",
    "    \n",
    "# remove mentions and URLs\n",
    "    text_noMentionURL = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", text)\n",
    "    text_noMentionURL = \" \".join(text_noMentionURL.split())\n",
    "    \n",
    "# remove '#' symbols and add space before capital letter\n",
    "    text_noHash = re.sub(r\"([A-Z]+)\", r\" \\1\", text_noMentionURL)\n",
    "    text_noHash = re.sub(r\"(#)\", \"\", text_noHash)\n",
    "    text_noHash = \" \".join(text_noHash.split())\n",
    "    \n",
    "# remove numbers\n",
    "    text_noHash = re.sub(r\"[0-9]+\", \"\", text_noHash)\n",
    "    \n",
    "# remove all other punctuation\n",
    "    text_noNoise = \"\".join([char for char in text_noHash if char not in string.punctuation])\n",
    "\n",
    "    return text_noNoise.lower()\n",
    "\n",
    "\n",
    "df['tweet_noNoise'] = df[\"text\"].apply(lambda x: preprocess_tweets(x))\n",
    "\n",
    "\n",
    "def no_smash(text):\n",
    "    \n",
    "    no_smash = re.sub(r\"(?:(?![aeiou])[a-z]){5}\", r\"\", text)\n",
    "    no_smash = \" \".join(no_smash.split())\n",
    "    \n",
    "    return no_smash\n",
    "    \n",
    "df['tweet_noNoise'] = df[\"tweet_noNoise\"].apply(lambda x: no_smash(x))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tweet_noNoise</th>\n",
       "      <th>lemmatized_list</th>\n",
       "      <th>old_processed_tweet</th>\n",
       "      <th>processed_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>[deed, reason, earthquake, may, allah, forgive...</td>\n",
       "      <td>[deed,reason,earthquake,may,allah,forgive,u]</td>\n",
       "      <td>deed,reason,earthquake,may,allah,forgive,u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>[forest,fire,near,la,ronge,sask,canada]</td>\n",
       "      <td>forest,fire,near,la,ronge,sask,canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>[resident, ask, shelter, place, notify, office...</td>\n",
       "      <td>[resident,ask,shelter,place,notify,officer,eva...</td>\n",
       "      <td>resident,ask,shelter,place,notify,officer,evac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfires evacuation orders in ...</td>\n",
       "      <td>[people, receive, wildfire, evacuation, order,...</td>\n",
       "      <td>[people,receive,wildfire,evacuation,order,cali...</td>\n",
       "      <td>people,receive,wildfire,evacuation,order,calif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>[get, send, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "      <td>[get,send,photo,ruby,alaska,smoke,wildfire,pou...</td>\n",
       "      <td>get,send,photo,ruby,alaska,smoke,wildfire,pour...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                      tweet_noNoise  \\\n",
       "0       1  our deeds are the reason of this earthquake ma...   \n",
       "1       1              forest fire near la ronge sask canada   \n",
       "2       1  all residents asked to shelter in place are be...   \n",
       "3       1  people receive wildfires evacuation orders in ...   \n",
       "4       1  just got sent this photo from ruby alaska as s...   \n",
       "\n",
       "                                     lemmatized_list  \\\n",
       "0  [deed, reason, earthquake, may, allah, forgive...   \n",
       "1      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2  [resident, ask, shelter, place, notify, office...   \n",
       "3  [people, receive, wildfire, evacuation, order,...   \n",
       "4  [get, send, photo, ruby, alaska, smoke, wildfi...   \n",
       "\n",
       "                                 old_processed_tweet  \\\n",
       "0       [deed,reason,earthquake,may,allah,forgive,u]   \n",
       "1            [forest,fire,near,la,ronge,sask,canada]   \n",
       "2  [resident,ask,shelter,place,notify,officer,eva...   \n",
       "3  [people,receive,wildfire,evacuation,order,cali...   \n",
       "4  [get,send,photo,ruby,alaska,smoke,wildfire,pou...   \n",
       "\n",
       "                                     processed_tweet  \n",
       "0         deed,reason,earthquake,may,allah,forgive,u  \n",
       "1              forest,fire,near,la,ronge,sask,canada  \n",
       "2  resident,ask,shelter,place,notify,officer,evac...  \n",
       "3  people,receive,wildfire,evacuation,order,calif...  \n",
       "4  get,send,photo,ruby,alaska,smoke,wildfire,pour...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatizer with Part-of-Speech (POS)\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# this function can be called within lemmatize() to tag the word with its POS\n",
    "def get_pos(word):\n",
    "    probable_part_of_speech = wordnet.synsets(word)\n",
    "  \n",
    "    pos_counts = Counter()\n",
    "\n",
    "    pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "    pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "    pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "    pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "  \n",
    "    most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "    return most_likely_part_of_speech\n",
    "\n",
    "def reduce_lengthening(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n",
    "\n",
    "# tokenize tweets = divide into words\n",
    "tokenized_tweets = df.apply(lambda row: tt.tokenize(row['tweet_noNoise']), axis=1)\n",
    "\n",
    "# lemmatize the list of all tokens\n",
    "df['lemmatized_list'] = tokenized_tweets.apply(lambda y: [lemmatizer.lemmatize(x, get_pos(x)) for x in y])\n",
    "df['lemmatized_list'].apply(lambda x: ' '.join([tweet for tweet in x]))\n",
    "\n",
    "# removes stop words (are, on, in, etc.) MUST COME BEFORE LIST -> STR\n",
    "stop = set(stopwords.words('english'))\n",
    "df['lemmatized_list'] = df['lemmatized_list'].apply(lambda x: [y for y in x if y not in stop])\n",
    "\n",
    "# turn the resulting list back into a string\n",
    "df['old_processed_tweet'] = df.lemmatized_list.agg(lambda x: ','.join(map(str, x)))\n",
    "\n",
    "# remove weird characters and 1 character after\n",
    "df['old_processed_tweet'] = df['old_processed_tweet'].replace({'û.':''}, regex=True)\n",
    "df['old_processed_tweet'] = df['old_processed_tweet'].replace({'ì.':''}, regex=True)\n",
    "df['old_processed_tweet'] = df['old_processed_tweet'].replace({'å.':''}, regex=True)\n",
    "df['old_processed_tweet'] = df['old_processed_tweet'].replace({'â.':''}, regex=True)\n",
    "\n",
    "df['old_processed_tweet'] = df['old_processed_tweet'].apply(lambda x: [reduce_lengthening(x)])\n",
    "df['processed_tweet'] = df.old_processed_tweet.agg(lambda x: ','.join(map(str, x)))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/generic.py:5516: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tweet_noNoise</th>\n",
       "      <th>lemmatized_list</th>\n",
       "      <th>old_processed_tweet</th>\n",
       "      <th>processed_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>725</td>\n",
       "      <td>attacked</td>\n",
       "      <td>LEALMAN, FLORIDA</td>\n",
       "      <td>Christian Attacked by Muslims at the Temple Mo...</td>\n",
       "      <td>1</td>\n",
       "      <td>christian attacked by muslims at the temple mo...</td>\n",
       "      <td>[christian, attack, muslim, temple, mount, wav...</td>\n",
       "      <td>[christian,attack,muslim,temple,mount,wave,isr...</td>\n",
       "      <td>christian,attack,muslim,temple,mount,wave,isra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>726</td>\n",
       "      <td>attacked</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>@envw98 @NickCoCoFree @JulieDiCaro @jdabe80 Wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>why am i the worst person questioning how juli...</td>\n",
       "      <td>[worst, person, question, julie, attack, guy, ...</td>\n",
       "      <td>[worst,person,question,julie,attack,guy,empathy]</td>\n",
       "      <td>worst,person,question,julie,attack,guy,empathy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>727</td>\n",
       "      <td>attacked</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>Kelly Osbourne attacked for racist Donald Trum...</td>\n",
       "      <td>1</td>\n",
       "      <td>kelly osbourne attacked for racist donald trum...</td>\n",
       "      <td>[kelly, osbourne, attack, racist, donald, trum...</td>\n",
       "      <td>[kelly,osbourne,attack,racist,donald,trump,rem...</td>\n",
       "      <td>kelly,osbourne,attack,racist,donald,trump,rema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>728</td>\n",
       "      <td>attacked</td>\n",
       "      <td>#GDJB #ASOT</td>\n",
       "      <td>@eunice_njoki aiii she needs to chill and answ...</td>\n",
       "      <td>0</td>\n",
       "      <td>aiii she needs to chill and answer calmly its ...</td>\n",
       "      <td>[aiii, need, chill, answer, calmly, like, shes...</td>\n",
       "      <td>[aii,need,chill,answer,calmly,like,shes,attack]</td>\n",
       "      <td>aii,need,chill,answer,calmly,like,shes,attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>729</td>\n",
       "      <td>attacked</td>\n",
       "      <td>Groningen, Netherlands, Europe</td>\n",
       "      <td>Christian Attacked by Muslims at the Temple Mo...</td>\n",
       "      <td>1</td>\n",
       "      <td>christian attacked by muslims at the temple mo...</td>\n",
       "      <td>[christian, attack, muslim, temple, mount, wav...</td>\n",
       "      <td>[christian,attack,muslim,temple,mount,wave,isr...</td>\n",
       "      <td>christian,attack,muslim,temple,mount,wave,isra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>730</td>\n",
       "      <td>attacked</td>\n",
       "      <td>Livingston, IL  U.S.A.</td>\n",
       "      <td>Christian Attacked by Muslims at the Temple Mo...</td>\n",
       "      <td>1</td>\n",
       "      <td>christian attacked by muslims at the temple mo...</td>\n",
       "      <td>[christian, attack, muslim, temple, mount, wav...</td>\n",
       "      <td>[christian,attack,muslim,temple,mount,wave,isr...</td>\n",
       "      <td>christian,attack,muslim,temple,mount,wave,isra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>731</td>\n",
       "      <td>attacked</td>\n",
       "      <td>Arundel</td>\n",
       "      <td>Christian Attacked by Muslims at the Temple Mo...</td>\n",
       "      <td>1</td>\n",
       "      <td>christian attacked by muslims at the temple mo...</td>\n",
       "      <td>[christian, attack, muslim, temple, mount, wav...</td>\n",
       "      <td>[christian,attack,muslim,temple,mount,wave,isr...</td>\n",
       "      <td>christian,attack,muslim,temple,mount,wave,isra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>732</td>\n",
       "      <td>attacked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I attacked Robot-lvl 19 and I've earned a tota...</td>\n",
       "      <td>0</td>\n",
       "      <td>i attacked robotlvl and ive earned a total of ...</td>\n",
       "      <td>[attack, robotlvl, ive, earn, total, free, sat...</td>\n",
       "      <td>[attack,robotlvl,ive,earn,total,free,satoshis,...</td>\n",
       "      <td>attack,robotlvl,ive,earn,total,free,satoshis,r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>734</td>\n",
       "      <td>attacked</td>\n",
       "      <td>America</td>\n",
       "      <td>Christian Attacked by Muslims at the Temple Mo...</td>\n",
       "      <td>1</td>\n",
       "      <td>christian attacked by muslims at the temple mo...</td>\n",
       "      <td>[christian, attack, muslim, temple, mount, wav...</td>\n",
       "      <td>[christian,attack,muslim,temple,mount,wave,isr...</td>\n",
       "      <td>christian,attack,muslim,temple,mount,wave,isra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>735</td>\n",
       "      <td>attacked</td>\n",
       "      <td>Anna Maria, FL</td>\n",
       "      <td>@christinalavv @lindsay_wynn3 I just saw these...</td>\n",
       "      <td>0</td>\n",
       "      <td>i just saw these tweets and i feel really atta...</td>\n",
       "      <td>[saw, tweet, feel, really, attack]</td>\n",
       "      <td>[saw,tweet,feel,really,attack]</td>\n",
       "      <td>saw,tweet,feel,really,attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>736</td>\n",
       "      <td>attacked</td>\n",
       "      <td>USA</td>\n",
       "      <td>Christian Attacked by Muslims at the Temple Mo...</td>\n",
       "      <td>1</td>\n",
       "      <td>christian attacked by muslims at the temple mo...</td>\n",
       "      <td>[christian, attack, muslim, temple, mount, wav...</td>\n",
       "      <td>[christian,attack,muslim,temple,mount,wave,isr...</td>\n",
       "      <td>christian,attack,muslim,temple,mount,wave,isra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>737</td>\n",
       "      <td>attacked</td>\n",
       "      <td>israel</td>\n",
       "      <td>Christian Attacked by Muslims at the Temple Mo...</td>\n",
       "      <td>1</td>\n",
       "      <td>christian attacked by muslims at the temple mo...</td>\n",
       "      <td>[christian, attack, muslim, temple, mount, wav...</td>\n",
       "      <td>[christian,attack,muslim,temple,mount,wave,isr...</td>\n",
       "      <td>christian,attack,muslim,temple,mount,wave,isra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>738</td>\n",
       "      <td>attacked</td>\n",
       "      <td>The Hammock, FL, USA</td>\n",
       "      <td>Christian Attacked by Muslims at the Temple Mo...</td>\n",
       "      <td>1</td>\n",
       "      <td>christian attacked by muslims at the temple mo...</td>\n",
       "      <td>[christian, attack, muslim, temple, mount, wav...</td>\n",
       "      <td>[christian,attack,muslim,temple,mount,wave,isr...</td>\n",
       "      <td>christian,attack,muslim,temple,mount,wave,isra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>739</td>\n",
       "      <td>attacked</td>\n",
       "      <td>??????????????????</td>\n",
       "      <td>TV program I saw said US air plane flew to ura...</td>\n",
       "      <td>1</td>\n",
       "      <td>tv program i saw said us air plane flew to ura...</td>\n",
       "      <td>[tv, program, saw, say, u, air, plane, fly, ur...</td>\n",
       "      <td>[tv,program,saw,say,u,air,plane,fly,uranium,mi...</td>\n",
       "      <td>tv,program,saw,say,u,air,plane,fly,uranium,min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>740</td>\n",
       "      <td>attacked</td>\n",
       "      <td>SÌ£o Paulo SP,  Brasil</td>\n",
       "      <td>Christian Attacked by Muslims at the Temple Mo...</td>\n",
       "      <td>1</td>\n",
       "      <td>christian attacked by muslims at the temple mo...</td>\n",
       "      <td>[christian, attack, muslim, temple, mount, wav...</td>\n",
       "      <td>[christian,attack,muslim,temple,mount,wave,isr...</td>\n",
       "      <td>christian,attack,muslim,temple,mount,wave,isra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>744</td>\n",
       "      <td>attacked</td>\n",
       "      <td>in Dimitri's arms</td>\n",
       "      <td>@MageAvexis &amp;lt; things. And what if we get at...</td>\n",
       "      <td>0</td>\n",
       "      <td>lt things and what if we get attacked</td>\n",
       "      <td>[lt, thing, get, attack]</td>\n",
       "      <td>[lt,thing,get,attack]</td>\n",
       "      <td>lt,thing,get,attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>745</td>\n",
       "      <td>attacked</td>\n",
       "      <td>Oslo, Norway</td>\n",
       "      <td>Christian Attacked by Muslims at the Temple Mo...</td>\n",
       "      <td>1</td>\n",
       "      <td>christian attacked by muslims at the temple mo...</td>\n",
       "      <td>[christian, attack, muslim, temple, mount, wav...</td>\n",
       "      <td>[christian,attack,muslim,temple,mount,wave,isr...</td>\n",
       "      <td>christian,attack,muslim,temple,mount,wave,isra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>746</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>#WeLoveLA #NHLDucks Avalanche Defense: How The...</td>\n",
       "      <td>0</td>\n",
       "      <td>we love la nhlducks avalanche defense how they...</td>\n",
       "      <td>[love, la, nhlducks, avalanche, defense, match...</td>\n",
       "      <td>[love,la,nhlducks,avalanche,defense,match,v,st...</td>\n",
       "      <td>love,la,nhlducks,avalanche,defense,match,v,st,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>748</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>Loughton, Essex, UK</td>\n",
       "      <td>I liked a @YouTube video http://t.co/TNXQuOr1w...</td>\n",
       "      <td>0</td>\n",
       "      <td>i liked a video kalle mattson avalanche offici...</td>\n",
       "      <td>[like, video, kalle, mattson, avalanche, offic...</td>\n",
       "      <td>[like,video,kalle,mattson,avalanche,official,v...</td>\n",
       "      <td>like,video,kalle,mattson,avalanche,official,video</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>751</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>guaravitas</td>\n",
       "      <td>we'll crash down like an avalanche</td>\n",
       "      <td>0</td>\n",
       "      <td>well crash down like an avalanche</td>\n",
       "      <td>[well, crash, like, avalanche]</td>\n",
       "      <td>[well,crash,like,avalanche]</td>\n",
       "      <td>well,crash,like,avalanche</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>752</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#Colorado #Avalanche Men's Official Colorado A...</td>\n",
       "      <td>0</td>\n",
       "      <td>colorado avalanche mens official colorado aval...</td>\n",
       "      <td>[colorado, avalanche, men, official, colorado,...</td>\n",
       "      <td>[colorado,avalanche,men,official,colorado,aval...</td>\n",
       "      <td>colorado,avalanche,men,official,colorado,avala...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>753</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>Score More Goals Buying @</td>\n",
       "      <td>2 TIX 10/3 Frozen Fury XVII: Los Angeles Kings...</td>\n",
       "      <td>0</td>\n",
       "      <td>tix frozen fury xvii los angeles kings v avala...</td>\n",
       "      <td>[tix, freeze, fury, xvii, los, angeles, king, ...</td>\n",
       "      <td>[tix,freeze,fury,xvii,los,angeles,king,v,avala...</td>\n",
       "      <td>tix,freeze,fury,xvii,los,angeles,king,v,avalan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>754</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>NEW YORK</td>\n",
       "      <td>I BET YOU DIDNT KNOW I KICK BOX TOO! https://t...</td>\n",
       "      <td>0</td>\n",
       "      <td>i bet you didnt know i kick box too</td>\n",
       "      <td>[bet, didnt, know, kick, box]</td>\n",
       "      <td>[bet,didnt,know,kick,box]</td>\n",
       "      <td>bet,didnt,know,kick,box</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>755</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>A little piece I wrote for the Avalanche Desig...</td>\n",
       "      <td>0</td>\n",
       "      <td>a little piece i wrote for the avalanche desig...</td>\n",
       "      <td>[little, piece, write, avalanche, design, blog...</td>\n",
       "      <td>[little,piece,write,avalanche,design,blog,id,a...</td>\n",
       "      <td>little,piece,write,avalanche,design,blog,id,ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>758</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PATRICK ROY 1998-99 UPPER DECK SPX #171 FINITE...</td>\n",
       "      <td>0</td>\n",
       "      <td>patrick roy upper deck spx finite made colorad...</td>\n",
       "      <td>[patrick, roy, upper, deck, spx, finite, make,...</td>\n",
       "      <td>[patrick,roy,upper,deck,spx,finite,make,colora...</td>\n",
       "      <td>patrick,roy,upper,deck,spx,finite,make,colorad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>759</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>UK</td>\n",
       "      <td>Musician Kalle Mattson Recreates 34 Classic Al...</td>\n",
       "      <td>0</td>\n",
       "      <td>musician kalle mattson recreates classic album...</td>\n",
       "      <td>[musician, kalle, mattson, recreate, classic, ...</td>\n",
       "      <td>[musician,kalle,mattson,recreate,classic,album...</td>\n",
       "      <td>musician,kalle,mattson,recreate,classic,album,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>761</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>London, Kent &amp; SE England.</td>\n",
       "      <td>Beautiful Sweet Avalanche Faith and Akito rose...</td>\n",
       "      <td>0</td>\n",
       "      <td>beautiful sweet avalanche faith and akito rose...</td>\n",
       "      <td>[beautiful, sweet, avalanche, faith, akito, ro...</td>\n",
       "      <td>[beautiful,sweet,avalanche,faith,akito,rose,lo...</td>\n",
       "      <td>beautiful,sweet,avalanche,faith,akito,rose,lot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>762</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>Score Team Goals Buying @</td>\n",
       "      <td>1-6 TIX Calgary Flames vs COL Avalanche Presea...</td>\n",
       "      <td>0</td>\n",
       "      <td>tix calgary flames vs col avalanche preseason ...</td>\n",
       "      <td>[tix, calgary, flame, v, col, avalanche, prese...</td>\n",
       "      <td>[tix,calgary,flame,v,col,avalanche,preseason,s...</td>\n",
       "      <td>tix,calgary,flame,v,col,avalanche,preseason,sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>763</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Secrets up avalanche: catechize inner self for...</td>\n",
       "      <td>0</td>\n",
       "      <td>secrets up avalanche catechize inner self for ...</td>\n",
       "      <td>[secret, avalanche, catechize, inner, self, co...</td>\n",
       "      <td>[secret,avalanche,catechize,inner,self,confide...</td>\n",
       "      <td>secret,avalanche,catechize,inner,self,confiden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>767</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>the fall of leaves from a poplar is as fully o...</td>\n",
       "      <td>0</td>\n",
       "      <td>the fall of leaves from a poplar is as fully o...</td>\n",
       "      <td>[fall, leave, poplar, fully, ordain, tumble, a...</td>\n",
       "      <td>[fall,leave,poplar,fully,ordain,tumble,avalanc...</td>\n",
       "      <td>fall,leave,poplar,fully,ordain,tumble,avalanch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>770</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>South Central Wales</td>\n",
       "      <td>I saw two great punk bands making original mus...</td>\n",
       "      <td>0</td>\n",
       "      <td>i saw two great punk bands making original mus...</td>\n",
       "      <td>[saw, two, great, punk, band, make, original, ...</td>\n",
       "      <td>[saw,two,great,punk,band,make,original,music,l...</td>\n",
       "      <td>saw,two,great,punk,band,make,original,music,la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>773</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GREAT PERFORMANCE CHIP FUEL/GAS SAVER CHEVY TA...</td>\n",
       "      <td>0</td>\n",
       "      <td>great performance chip fuel gas saver chevy ta...</td>\n",
       "      <td>[great, performance, chip, fuel, gas, saver, c...</td>\n",
       "      <td>[great,performance,chip,fuel,gas,saver,chevy,t...</td>\n",
       "      <td>great,performance,chip,fuel,gas,saver,chevy,ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>774</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>Jersey City, New Jersey</td>\n",
       "      <td>Musician Kalle Mattson Recreates 34 Classic Al...</td>\n",
       "      <td>0</td>\n",
       "      <td>musician kalle mattson recreates classic album...</td>\n",
       "      <td>[musician, kalle, mattson, recreate, classic, ...</td>\n",
       "      <td>[musician,kalle,mattson,recreate,classic,album...</td>\n",
       "      <td>musician,kalle,mattson,recreate,classic,album,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>775</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>NaN</td>\n",
       "      <td>driving the avalanche after having my car for ...</td>\n",
       "      <td>1</td>\n",
       "      <td>driving the avalanche after having my car for ...</td>\n",
       "      <td>[drive, avalanche, car, week, like, drive, tank]</td>\n",
       "      <td>[drive,avalanche,car,week,like,drive,tank]</td>\n",
       "      <td>drive,avalanche,car,week,like,drive,tank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>777</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Free Ebay Sniping RT? http://t.co/RqIPGQslT6 C...</td>\n",
       "      <td>0</td>\n",
       "      <td>free ebay sniping rt chevrolet avalanche ltz l...</td>\n",
       "      <td>[free, ebay, snip, rt, chevrolet, avalanche, l...</td>\n",
       "      <td>[free,ebay,snip,rt,chevrolet,avalanche,ltz,lif...</td>\n",
       "      <td>free,ebay,snip,rt,chevrolet,avalanche,ltz,lift...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>779</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>Denver, CO</td>\n",
       "      <td>Chiasson Sens can't come to deal #ColoradoAval...</td>\n",
       "      <td>1</td>\n",
       "      <td>chiasson sens cant come to deal colorado avala...</td>\n",
       "      <td>[chiasson, sen, cant, come, deal, colorado, av...</td>\n",
       "      <td>[chiasson,sen,cant,come,deal,colorado,avalanch...</td>\n",
       "      <td>chiasson,sen,cant,come,deal,colorado,avalanche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>781</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>Brasil</td>\n",
       "      <td>Paul Rudd Emile Hirsch David Gordon Green 'Pri...</td>\n",
       "      <td>0</td>\n",
       "      <td>paul rudd emile hirsch david gordon green prin...</td>\n",
       "      <td>[paul, rudd, emile, hirsch, david, gordon, gre...</td>\n",
       "      <td>[paul,rudd,emile,hirsch,david,gordon,green,pri...</td>\n",
       "      <td>paul,rudd,emile,hirsch,david,gordon,green,prin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>782</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>Buy Give Me My Money</td>\n",
       "      <td>Great one time deal on all Avalanche music and...</td>\n",
       "      <td>0</td>\n",
       "      <td>great one time deal on all avalanche music and...</td>\n",
       "      <td>[great, one, time, deal, avalanche, music, pur...</td>\n",
       "      <td>[great,one,time,deal,avalanche,music,purchase,...</td>\n",
       "      <td>great,one,time,deal,avalanche,music,purchase,g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>783</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>505 W. Maple, Suite 100</td>\n",
       "      <td>.@bigperm28 was drafted by the @Avalanche in 2...</td>\n",
       "      <td>0</td>\n",
       "      <td>was drafted by the in rd overall played last s...</td>\n",
       "      <td>[draft, rd, overall, play, last, season]</td>\n",
       "      <td>[draft,rd,overall,play,last,season]</td>\n",
       "      <td>draft,rd,overall,play,last,season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>784</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>Buy Give Me My Money</td>\n",
       "      <td>I HAVE GOT MORE VIDEOS THAN YOU RAPPERS GOT SO...</td>\n",
       "      <td>0</td>\n",
       "      <td>i have got more videos than you rappers got songs</td>\n",
       "      <td>[get, video, rapper, get, song]</td>\n",
       "      <td>[get,video,rapper,get,song]</td>\n",
       "      <td>get,video,rapper,get,song</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>786</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>Buy Give Me My Money</td>\n",
       "      <td>@funkflex yo flex im here https://t.co/2AZxdLCXgA</td>\n",
       "      <td>0</td>\n",
       "      <td>yo flex im here</td>\n",
       "      <td>[yo, flex, im]</td>\n",
       "      <td>[yo,flex,im]</td>\n",
       "      <td>yo,flex,im</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>787</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>Philippines</td>\n",
       "      <td>You are the avalanche. One world away. My make...</td>\n",
       "      <td>0</td>\n",
       "      <td>you are the avalanche one world away my make b...</td>\n",
       "      <td>[avalanche, one, world, away, make, believe, i...</td>\n",
       "      <td>[avalanche,one,world,away,make,believe,im,wide...</td>\n",
       "      <td>avalanche,one,world,away,make,believe,im,wide,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>788</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>Freeport il</td>\n",
       "      <td>The possible new jerseys for the Avalanche nex...</td>\n",
       "      <td>0</td>\n",
       "      <td>the possible new jerseys for the avalanche nex...</td>\n",
       "      <td>[possible, new, jersey, avalanche, next, year]</td>\n",
       "      <td>[possible,new,jersey,avalanche,next,year]</td>\n",
       "      <td>possible,new,jersey,avalanche,next,year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>790</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>Canada</td>\n",
       "      <td>What a feat! Watch the #BTS of @kallemattson's...</td>\n",
       "      <td>0</td>\n",
       "      <td>what a feat watch the bts of incredible music ...</td>\n",
       "      <td>[feat, watch, bts, incredible, music, video, a...</td>\n",
       "      <td>[feat,watch,bts,incredible,music,video,avalanche]</td>\n",
       "      <td>feat,watch,bts,incredible,music,video,avalanche</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>791</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>World</td>\n",
       "      <td>Avalanche City - Sunset http://t.co/48h3tLvLXr...</td>\n",
       "      <td>1</td>\n",
       "      <td>avalanche city sunset nowplay listen radio</td>\n",
       "      <td>[avalanche, city, sunset, nowplay, listen, radio]</td>\n",
       "      <td>[avalanche,city,sunset,nowplay,listen,radio]</td>\n",
       "      <td>avalanche,city,sunset,nowplay,listen,radio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>794</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chevrolet : Avalanche LT 2011 lt used 5.3 l v ...</td>\n",
       "      <td>1</td>\n",
       "      <td>chevrolet avalanche lt lt used l v v automatic...</td>\n",
       "      <td>[chevrolet, avalanche, lt, lt, use, l, v, v, a...</td>\n",
       "      <td>[chevrolet,avalanche,lt,lt,use,l,v,v,automatic...</td>\n",
       "      <td>chevrolet,avalanche,lt,lt,use,l,v,v,automatic,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>795</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>Danville, VA</td>\n",
       "      <td>No snowflake in an avalanche ever feels respon...</td>\n",
       "      <td>0</td>\n",
       "      <td>no snowflake in an avalanche ever feels respon...</td>\n",
       "      <td>[snowflake, avalanche, ever, feel, responsible]</td>\n",
       "      <td>[snowflake,avalanche,ever,feel,responsible]</td>\n",
       "      <td>snowflake,avalanche,ever,feel,responsible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>796</td>\n",
       "      <td>battle</td>\n",
       "      <td>New York</td>\n",
       "      <td>STAR WARS POWER OF THE JEDI COLLECTION 1 BATTL...</td>\n",
       "      <td>1</td>\n",
       "      <td>star wars power of the jedi collection battle ...</td>\n",
       "      <td>[star, war, power, jedi, collection, battle, d...</td>\n",
       "      <td>[star,war,power,jedi,collection,battle,droid,h...</td>\n",
       "      <td>star,war,power,jedi,collection,battle,droid,ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>797</td>\n",
       "      <td>battle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CIVIL WAR GENERAL BATTLE BULL RUN HERO COLONEL...</td>\n",
       "      <td>1</td>\n",
       "      <td>civil war general battle bull run hero colonel...</td>\n",
       "      <td>[civil, war, general, battle, bull, run, hero,...</td>\n",
       "      <td>[civil,war,general,battle,bull,run,hero,colone...</td>\n",
       "      <td>civil,war,general,battle,bull,run,hero,colonel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>798</td>\n",
       "      <td>battle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dragon Ball Z: Battle Of Gods (2014) - Rotten ...</td>\n",
       "      <td>0</td>\n",
       "      <td>dragon ball z battle of gods rotten tomatoes via</td>\n",
       "      <td>[dragon, ball, z, battle, god, rotten, tomato,...</td>\n",
       "      <td>[dragon,ball,z,battle,god,rotten,tomato,via]</td>\n",
       "      <td>dragon,ball,z,battle,god,rotten,tomato,via</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id    keyword                        location  \\\n",
       "500  725   attacked                LEALMAN, FLORIDA   \n",
       "501  726   attacked                 Los Angeles, CA   \n",
       "502  727   attacked               San Francisco, CA   \n",
       "503  728   attacked                     #GDJB #ASOT   \n",
       "504  729   attacked  Groningen, Netherlands, Europe   \n",
       "505  730   attacked          Livingston, IL  U.S.A.   \n",
       "506  731   attacked                        Arundel    \n",
       "507  732   attacked                             NaN   \n",
       "508  734   attacked                         America   \n",
       "509  735   attacked                  Anna Maria, FL   \n",
       "510  736   attacked                             USA   \n",
       "511  737   attacked                          israel   \n",
       "512  738   attacked            The Hammock, FL, USA   \n",
       "513  739   attacked              ??????????????????   \n",
       "514  740   attacked          SÌ£o Paulo SP,  Brasil   \n",
       "515  744   attacked               in Dimitri's arms   \n",
       "516  745   attacked                    Oslo, Norway   \n",
       "517  746  avalanche                     Los Angeles   \n",
       "518  748  avalanche             Loughton, Essex, UK   \n",
       "519  751  avalanche                      guaravitas   \n",
       "520  752  avalanche                             NaN   \n",
       "521  753  avalanche       Score More Goals Buying @   \n",
       "522  754  avalanche                        NEW YORK   \n",
       "523  755  avalanche                         Ireland   \n",
       "524  758  avalanche                             NaN   \n",
       "525  759  avalanche                              UK   \n",
       "526  761  avalanche      London, Kent & SE England.   \n",
       "527  762  avalanche       Score Team Goals Buying @   \n",
       "528  763  avalanche                             NaN   \n",
       "529  767  avalanche                    New York, NY   \n",
       "530  770  avalanche             South Central Wales   \n",
       "531  773  avalanche                             NaN   \n",
       "532  774  avalanche         Jersey City, New Jersey   \n",
       "533  775  avalanche                             NaN   \n",
       "534  777  avalanche                             NaN   \n",
       "535  779  avalanche                      Denver, CO   \n",
       "536  781  avalanche                          Brasil   \n",
       "537  782  avalanche           Buy Give Me My Money    \n",
       "538  783  avalanche         505 W. Maple, Suite 100   \n",
       "539  784  avalanche           Buy Give Me My Money    \n",
       "540  786  avalanche           Buy Give Me My Money    \n",
       "541  787  avalanche                     Philippines   \n",
       "542  788  avalanche                    Freeport il    \n",
       "543  790  avalanche                          Canada   \n",
       "544  791  avalanche                           World   \n",
       "545  794  avalanche                             NaN   \n",
       "546  795  avalanche                    Danville, VA   \n",
       "547  796     battle                        New York   \n",
       "548  797     battle                             NaN   \n",
       "549  798     battle                             NaN   \n",
       "\n",
       "                                                  text  target  \\\n",
       "500  Christian Attacked by Muslims at the Temple Mo...       1   \n",
       "501  @envw98 @NickCoCoFree @JulieDiCaro @jdabe80 Wh...       0   \n",
       "502  Kelly Osbourne attacked for racist Donald Trum...       1   \n",
       "503  @eunice_njoki aiii she needs to chill and answ...       0   \n",
       "504  Christian Attacked by Muslims at the Temple Mo...       1   \n",
       "505  Christian Attacked by Muslims at the Temple Mo...       1   \n",
       "506  Christian Attacked by Muslims at the Temple Mo...       1   \n",
       "507  I attacked Robot-lvl 19 and I've earned a tota...       0   \n",
       "508  Christian Attacked by Muslims at the Temple Mo...       1   \n",
       "509  @christinalavv @lindsay_wynn3 I just saw these...       0   \n",
       "510  Christian Attacked by Muslims at the Temple Mo...       1   \n",
       "511  Christian Attacked by Muslims at the Temple Mo...       1   \n",
       "512  Christian Attacked by Muslims at the Temple Mo...       1   \n",
       "513  TV program I saw said US air plane flew to ura...       1   \n",
       "514  Christian Attacked by Muslims at the Temple Mo...       1   \n",
       "515  @MageAvexis &lt; things. And what if we get at...       0   \n",
       "516  Christian Attacked by Muslims at the Temple Mo...       1   \n",
       "517  #WeLoveLA #NHLDucks Avalanche Defense: How The...       0   \n",
       "518  I liked a @YouTube video http://t.co/TNXQuOr1w...       0   \n",
       "519                 we'll crash down like an avalanche       0   \n",
       "520  #Colorado #Avalanche Men's Official Colorado A...       0   \n",
       "521  2 TIX 10/3 Frozen Fury XVII: Los Angeles Kings...       0   \n",
       "522  I BET YOU DIDNT KNOW I KICK BOX TOO! https://t...       0   \n",
       "523  A little piece I wrote for the Avalanche Desig...       0   \n",
       "524  PATRICK ROY 1998-99 UPPER DECK SPX #171 FINITE...       0   \n",
       "525  Musician Kalle Mattson Recreates 34 Classic Al...       0   \n",
       "526  Beautiful Sweet Avalanche Faith and Akito rose...       0   \n",
       "527  1-6 TIX Calgary Flames vs COL Avalanche Presea...       0   \n",
       "528  Secrets up avalanche: catechize inner self for...       0   \n",
       "529  the fall of leaves from a poplar is as fully o...       0   \n",
       "530  I saw two great punk bands making original mus...       0   \n",
       "531  GREAT PERFORMANCE CHIP FUEL/GAS SAVER CHEVY TA...       0   \n",
       "532  Musician Kalle Mattson Recreates 34 Classic Al...       0   \n",
       "533  driving the avalanche after having my car for ...       1   \n",
       "534  Free Ebay Sniping RT? http://t.co/RqIPGQslT6 C...       0   \n",
       "535  Chiasson Sens can't come to deal #ColoradoAval...       1   \n",
       "536  Paul Rudd Emile Hirsch David Gordon Green 'Pri...       0   \n",
       "537  Great one time deal on all Avalanche music and...       0   \n",
       "538  .@bigperm28 was drafted by the @Avalanche in 2...       0   \n",
       "539  I HAVE GOT MORE VIDEOS THAN YOU RAPPERS GOT SO...       0   \n",
       "540  @funkflex yo flex im here https://t.co/2AZxdLCXgA       0   \n",
       "541  You are the avalanche. One world away. My make...       0   \n",
       "542  The possible new jerseys for the Avalanche nex...       0   \n",
       "543  What a feat! Watch the #BTS of @kallemattson's...       0   \n",
       "544  Avalanche City - Sunset http://t.co/48h3tLvLXr...       1   \n",
       "545  Chevrolet : Avalanche LT 2011 lt used 5.3 l v ...       1   \n",
       "546  No snowflake in an avalanche ever feels respon...       0   \n",
       "547  STAR WARS POWER OF THE JEDI COLLECTION 1 BATTL...       1   \n",
       "548  CIVIL WAR GENERAL BATTLE BULL RUN HERO COLONEL...       1   \n",
       "549  Dragon Ball Z: Battle Of Gods (2014) - Rotten ...       0   \n",
       "\n",
       "                                         tweet_noNoise  \\\n",
       "500  christian attacked by muslims at the temple mo...   \n",
       "501  why am i the worst person questioning how juli...   \n",
       "502  kelly osbourne attacked for racist donald trum...   \n",
       "503  aiii she needs to chill and answer calmly its ...   \n",
       "504  christian attacked by muslims at the temple mo...   \n",
       "505  christian attacked by muslims at the temple mo...   \n",
       "506  christian attacked by muslims at the temple mo...   \n",
       "507  i attacked robotlvl and ive earned a total of ...   \n",
       "508  christian attacked by muslims at the temple mo...   \n",
       "509  i just saw these tweets and i feel really atta...   \n",
       "510  christian attacked by muslims at the temple mo...   \n",
       "511  christian attacked by muslims at the temple mo...   \n",
       "512  christian attacked by muslims at the temple mo...   \n",
       "513  tv program i saw said us air plane flew to ura...   \n",
       "514  christian attacked by muslims at the temple mo...   \n",
       "515              lt things and what if we get attacked   \n",
       "516  christian attacked by muslims at the temple mo...   \n",
       "517  we love la nhlducks avalanche defense how they...   \n",
       "518  i liked a video kalle mattson avalanche offici...   \n",
       "519                  well crash down like an avalanche   \n",
       "520  colorado avalanche mens official colorado aval...   \n",
       "521  tix frozen fury xvii los angeles kings v avala...   \n",
       "522                i bet you didnt know i kick box too   \n",
       "523  a little piece i wrote for the avalanche desig...   \n",
       "524  patrick roy upper deck spx finite made colorad...   \n",
       "525  musician kalle mattson recreates classic album...   \n",
       "526  beautiful sweet avalanche faith and akito rose...   \n",
       "527  tix calgary flames vs col avalanche preseason ...   \n",
       "528  secrets up avalanche catechize inner self for ...   \n",
       "529  the fall of leaves from a poplar is as fully o...   \n",
       "530  i saw two great punk bands making original mus...   \n",
       "531  great performance chip fuel gas saver chevy ta...   \n",
       "532  musician kalle mattson recreates classic album...   \n",
       "533  driving the avalanche after having my car for ...   \n",
       "534  free ebay sniping rt chevrolet avalanche ltz l...   \n",
       "535  chiasson sens cant come to deal colorado avala...   \n",
       "536  paul rudd emile hirsch david gordon green prin...   \n",
       "537  great one time deal on all avalanche music and...   \n",
       "538  was drafted by the in rd overall played last s...   \n",
       "539  i have got more videos than you rappers got songs   \n",
       "540                                    yo flex im here   \n",
       "541  you are the avalanche one world away my make b...   \n",
       "542  the possible new jerseys for the avalanche nex...   \n",
       "543  what a feat watch the bts of incredible music ...   \n",
       "544         avalanche city sunset nowplay listen radio   \n",
       "545  chevrolet avalanche lt lt used l v v automatic...   \n",
       "546  no snowflake in an avalanche ever feels respon...   \n",
       "547  star wars power of the jedi collection battle ...   \n",
       "548  civil war general battle bull run hero colonel...   \n",
       "549   dragon ball z battle of gods rotten tomatoes via   \n",
       "\n",
       "                                       lemmatized_list  \\\n",
       "500  [christian, attack, muslim, temple, mount, wav...   \n",
       "501  [worst, person, question, julie, attack, guy, ...   \n",
       "502  [kelly, osbourne, attack, racist, donald, trum...   \n",
       "503  [aiii, need, chill, answer, calmly, like, shes...   \n",
       "504  [christian, attack, muslim, temple, mount, wav...   \n",
       "505  [christian, attack, muslim, temple, mount, wav...   \n",
       "506  [christian, attack, muslim, temple, mount, wav...   \n",
       "507  [attack, robotlvl, ive, earn, total, free, sat...   \n",
       "508  [christian, attack, muslim, temple, mount, wav...   \n",
       "509                 [saw, tweet, feel, really, attack]   \n",
       "510  [christian, attack, muslim, temple, mount, wav...   \n",
       "511  [christian, attack, muslim, temple, mount, wav...   \n",
       "512  [christian, attack, muslim, temple, mount, wav...   \n",
       "513  [tv, program, saw, say, u, air, plane, fly, ur...   \n",
       "514  [christian, attack, muslim, temple, mount, wav...   \n",
       "515                           [lt, thing, get, attack]   \n",
       "516  [christian, attack, muslim, temple, mount, wav...   \n",
       "517  [love, la, nhlducks, avalanche, defense, match...   \n",
       "518  [like, video, kalle, mattson, avalanche, offic...   \n",
       "519                     [well, crash, like, avalanche]   \n",
       "520  [colorado, avalanche, men, official, colorado,...   \n",
       "521  [tix, freeze, fury, xvii, los, angeles, king, ...   \n",
       "522                      [bet, didnt, know, kick, box]   \n",
       "523  [little, piece, write, avalanche, design, blog...   \n",
       "524  [patrick, roy, upper, deck, spx, finite, make,...   \n",
       "525  [musician, kalle, mattson, recreate, classic, ...   \n",
       "526  [beautiful, sweet, avalanche, faith, akito, ro...   \n",
       "527  [tix, calgary, flame, v, col, avalanche, prese...   \n",
       "528  [secret, avalanche, catechize, inner, self, co...   \n",
       "529  [fall, leave, poplar, fully, ordain, tumble, a...   \n",
       "530  [saw, two, great, punk, band, make, original, ...   \n",
       "531  [great, performance, chip, fuel, gas, saver, c...   \n",
       "532  [musician, kalle, mattson, recreate, classic, ...   \n",
       "533   [drive, avalanche, car, week, like, drive, tank]   \n",
       "534  [free, ebay, snip, rt, chevrolet, avalanche, l...   \n",
       "535  [chiasson, sen, cant, come, deal, colorado, av...   \n",
       "536  [paul, rudd, emile, hirsch, david, gordon, gre...   \n",
       "537  [great, one, time, deal, avalanche, music, pur...   \n",
       "538           [draft, rd, overall, play, last, season]   \n",
       "539                    [get, video, rapper, get, song]   \n",
       "540                                     [yo, flex, im]   \n",
       "541  [avalanche, one, world, away, make, believe, i...   \n",
       "542     [possible, new, jersey, avalanche, next, year]   \n",
       "543  [feat, watch, bts, incredible, music, video, a...   \n",
       "544  [avalanche, city, sunset, nowplay, listen, radio]   \n",
       "545  [chevrolet, avalanche, lt, lt, use, l, v, v, a...   \n",
       "546    [snowflake, avalanche, ever, feel, responsible]   \n",
       "547  [star, war, power, jedi, collection, battle, d...   \n",
       "548  [civil, war, general, battle, bull, run, hero,...   \n",
       "549  [dragon, ball, z, battle, god, rotten, tomato,...   \n",
       "\n",
       "                                   old_processed_tweet  \\\n",
       "500  [christian,attack,muslim,temple,mount,wave,isr...   \n",
       "501   [worst,person,question,julie,attack,guy,empathy]   \n",
       "502  [kelly,osbourne,attack,racist,donald,trump,rem...   \n",
       "503    [aii,need,chill,answer,calmly,like,shes,attack]   \n",
       "504  [christian,attack,muslim,temple,mount,wave,isr...   \n",
       "505  [christian,attack,muslim,temple,mount,wave,isr...   \n",
       "506  [christian,attack,muslim,temple,mount,wave,isr...   \n",
       "507  [attack,robotlvl,ive,earn,total,free,satoshis,...   \n",
       "508  [christian,attack,muslim,temple,mount,wave,isr...   \n",
       "509                     [saw,tweet,feel,really,attack]   \n",
       "510  [christian,attack,muslim,temple,mount,wave,isr...   \n",
       "511  [christian,attack,muslim,temple,mount,wave,isr...   \n",
       "512  [christian,attack,muslim,temple,mount,wave,isr...   \n",
       "513  [tv,program,saw,say,u,air,plane,fly,uranium,mi...   \n",
       "514  [christian,attack,muslim,temple,mount,wave,isr...   \n",
       "515                              [lt,thing,get,attack]   \n",
       "516  [christian,attack,muslim,temple,mount,wave,isr...   \n",
       "517  [love,la,nhlducks,avalanche,defense,match,v,st...   \n",
       "518  [like,video,kalle,mattson,avalanche,official,v...   \n",
       "519                        [well,crash,like,avalanche]   \n",
       "520  [colorado,avalanche,men,official,colorado,aval...   \n",
       "521  [tix,freeze,fury,xvii,los,angeles,king,v,avala...   \n",
       "522                          [bet,didnt,know,kick,box]   \n",
       "523  [little,piece,write,avalanche,design,blog,id,a...   \n",
       "524  [patrick,roy,upper,deck,spx,finite,make,colora...   \n",
       "525  [musician,kalle,mattson,recreate,classic,album...   \n",
       "526  [beautiful,sweet,avalanche,faith,akito,rose,lo...   \n",
       "527  [tix,calgary,flame,v,col,avalanche,preseason,s...   \n",
       "528  [secret,avalanche,catechize,inner,self,confide...   \n",
       "529  [fall,leave,poplar,fully,ordain,tumble,avalanc...   \n",
       "530  [saw,two,great,punk,band,make,original,music,l...   \n",
       "531  [great,performance,chip,fuel,gas,saver,chevy,t...   \n",
       "532  [musician,kalle,mattson,recreate,classic,album...   \n",
       "533         [drive,avalanche,car,week,like,drive,tank]   \n",
       "534  [free,ebay,snip,rt,chevrolet,avalanche,ltz,lif...   \n",
       "535  [chiasson,sen,cant,come,deal,colorado,avalanch...   \n",
       "536  [paul,rudd,emile,hirsch,david,gordon,green,pri...   \n",
       "537  [great,one,time,deal,avalanche,music,purchase,...   \n",
       "538                [draft,rd,overall,play,last,season]   \n",
       "539                        [get,video,rapper,get,song]   \n",
       "540                                       [yo,flex,im]   \n",
       "541  [avalanche,one,world,away,make,believe,im,wide...   \n",
       "542          [possible,new,jersey,avalanche,next,year]   \n",
       "543  [feat,watch,bts,incredible,music,video,avalanche]   \n",
       "544       [avalanche,city,sunset,nowplay,listen,radio]   \n",
       "545  [chevrolet,avalanche,lt,lt,use,l,v,v,automatic...   \n",
       "546        [snowflake,avalanche,ever,feel,responsible]   \n",
       "547  [star,war,power,jedi,collection,battle,droid,h...   \n",
       "548  [civil,war,general,battle,bull,run,hero,colone...   \n",
       "549       [dragon,ball,z,battle,god,rotten,tomato,via]   \n",
       "\n",
       "                                       processed_tweet  \n",
       "500  christian,attack,muslim,temple,mount,wave,isra...  \n",
       "501     worst,person,question,julie,attack,guy,empathy  \n",
       "502  kelly,osbourne,attack,racist,donald,trump,rema...  \n",
       "503      aii,need,chill,answer,calmly,like,shes,attack  \n",
       "504  christian,attack,muslim,temple,mount,wave,isra...  \n",
       "505  christian,attack,muslim,temple,mount,wave,isra...  \n",
       "506  christian,attack,muslim,temple,mount,wave,isra...  \n",
       "507  attack,robotlvl,ive,earn,total,free,satoshis,r...  \n",
       "508  christian,attack,muslim,temple,mount,wave,isra...  \n",
       "509                       saw,tweet,feel,really,attack  \n",
       "510  christian,attack,muslim,temple,mount,wave,isra...  \n",
       "511  christian,attack,muslim,temple,mount,wave,isra...  \n",
       "512  christian,attack,muslim,temple,mount,wave,isra...  \n",
       "513  tv,program,saw,say,u,air,plane,fly,uranium,min...  \n",
       "514  christian,attack,muslim,temple,mount,wave,isra...  \n",
       "515                                lt,thing,get,attack  \n",
       "516  christian,attack,muslim,temple,mount,wave,isra...  \n",
       "517  love,la,nhlducks,avalanche,defense,match,v,st,...  \n",
       "518  like,video,kalle,mattson,avalanche,official,video  \n",
       "519                          well,crash,like,avalanche  \n",
       "520  colorado,avalanche,men,official,colorado,avala...  \n",
       "521  tix,freeze,fury,xvii,los,angeles,king,v,avalan...  \n",
       "522                            bet,didnt,know,kick,box  \n",
       "523  little,piece,write,avalanche,design,blog,id,ap...  \n",
       "524  patrick,roy,upper,deck,spx,finite,make,colorad...  \n",
       "525  musician,kalle,mattson,recreate,classic,album,...  \n",
       "526  beautiful,sweet,avalanche,faith,akito,rose,lot...  \n",
       "527  tix,calgary,flame,v,col,avalanche,preseason,sc...  \n",
       "528  secret,avalanche,catechize,inner,self,confiden...  \n",
       "529  fall,leave,poplar,fully,ordain,tumble,avalanch...  \n",
       "530  saw,two,great,punk,band,make,original,music,la...  \n",
       "531  great,performance,chip,fuel,gas,saver,chevy,ta...  \n",
       "532  musician,kalle,mattson,recreate,classic,album,...  \n",
       "533           drive,avalanche,car,week,like,drive,tank  \n",
       "534  free,ebay,snip,rt,chevrolet,avalanche,ltz,lift...  \n",
       "535  chiasson,sen,cant,come,deal,colorado,avalanche...  \n",
       "536  paul,rudd,emile,hirsch,david,gordon,green,prin...  \n",
       "537  great,one,time,deal,avalanche,music,purchase,g...  \n",
       "538                  draft,rd,overall,play,last,season  \n",
       "539                          get,video,rapper,get,song  \n",
       "540                                         yo,flex,im  \n",
       "541  avalanche,one,world,away,make,believe,im,wide,...  \n",
       "542            possible,new,jersey,avalanche,next,year  \n",
       "543    feat,watch,bts,incredible,music,video,avalanche  \n",
       "544         avalanche,city,sunset,nowplay,listen,radio  \n",
       "545  chevrolet,avalanche,lt,lt,use,l,v,v,automatic,...  \n",
       "546          snowflake,avalanche,ever,feel,responsible  \n",
       "547  star,war,power,jedi,collection,battle,droid,ha...  \n",
       "548  civil,war,general,battle,bull,run,hero,colonel...  \n",
       "549         dragon,ball,z,battle,god,rotten,tomato,via  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()\n",
    "def spell_check(text):\n",
    "    \n",
    "    text_noMiss = re.sub(text, spell.correction(text), text)\n",
    "    text_noMiss = \" \".join(text_noMiss.split())\n",
    "    \n",
    "    return text_noMiss\n",
    "testing = df.loc[500:550]   \n",
    "testing.processed_tweet = testing.processed_tweet.apply(lambda txt: spell_check(txt))\n",
    "testing.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weed\n"
     ]
    }
   ],
   "source": [
    "# enter whatever word you'd like to see how lemmatizing works\n",
    "print(lemmatizer.lemmatize('weeds'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         level_0   aa  aahh  aal  aall  aamp  aan  aand  aar  aashiqui  ...  \\\n",
      "0        Tweet 1  0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0       0.0  ...   \n",
      "1        Tweet 2  0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0       0.0  ...   \n",
      "2        Tweet 3  0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0       0.0  ...   \n",
      "3        Tweet 4  0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0       0.0  ...   \n",
      "4        Tweet 5  0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0       0.0  ...   \n",
      "...          ...  ...   ...  ...   ...   ...  ...   ...  ...       ...  ...   \n",
      "7608  Tweet 7609  0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0       0.0  ...   \n",
      "7609  Tweet 7610  0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0       0.0  ...   \n",
      "7610  Tweet 7611  0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0       0.0  ...   \n",
      "7611  Tweet 7612  0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0       0.0  ...   \n",
      "7612  Tweet 7613  0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0       0.0  ...   \n",
      "\n",
      "      zonesthank  zonewolf  zoom  zotar  zouma  zqp   zr  zumiez  zurich   zz  \n",
      "0            0.0       0.0   0.0    0.0    0.0  0.0  0.0     0.0     0.0  0.0  \n",
      "1            0.0       0.0   0.0    0.0    0.0  0.0  0.0     0.0     0.0  0.0  \n",
      "2            0.0       0.0   0.0    0.0    0.0  0.0  0.0     0.0     0.0  0.0  \n",
      "3            0.0       0.0   0.0    0.0    0.0  0.0  0.0     0.0     0.0  0.0  \n",
      "4            0.0       0.0   0.0    0.0    0.0  0.0  0.0     0.0     0.0  0.0  \n",
      "...          ...       ...   ...    ...    ...  ...  ...     ...     ...  ...  \n",
      "7608         0.0       0.0   0.0    0.0    0.0  0.0  0.0     0.0     0.0  0.0  \n",
      "7609         0.0       0.0   0.0    0.0    0.0  0.0  0.0     0.0     0.0  0.0  \n",
      "7610         0.0       0.0   0.0    0.0    0.0  0.0  0.0     0.0     0.0  0.0  \n",
      "7611         0.0       0.0   0.0    0.0    0.0  0.0  0.0     0.0     0.0  0.0  \n",
      "7612         0.0       0.0   0.0    0.0    0.0  0.0  0.0     0.0     0.0  0.0  \n",
      "\n",
      "[7613 rows x 11574 columns]\n"
     ]
    }
   ],
   "source": [
    "# first attempts at tf-idf\n",
    "# input needs to exclude mentions & hashtags, be in lower case, remove punct, lemmatized\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(norm=None)\n",
    "tfidf_scores = vectorizer.fit_transform(df.processed_tweet)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# get corpus index\n",
    "corpus_index = [f\"Tweet {i+1}\" for i in range(len(df.processed_tweet))]\n",
    "\n",
    "# create pandas DataFrame with tf-idf scores\n",
    "df_tf_idf = pd.DataFrame(tfidf_scores.T.todense(), index=feature_names, columns=corpus_index)\n",
    "# df_tf_idf.reset_index()\n",
    "\n",
    "df_tf_idf = df_tf_idf.T.reset_index()\n",
    "\n",
    "print(df_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      target\n",
      "0          1\n",
      "1          1\n",
      "2          1\n",
      "3          1\n",
      "4          1\n",
      "...      ...\n",
      "7608       1\n",
      "7609       1\n",
      "7610       1\n",
      "7611       1\n",
      "7612       1\n",
      "\n",
      "[7613 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# put targets into their own dataframe so i can merge them with tf-idf scores\n",
    "target_df = pd.DataFrame()\n",
    "target_df['target'] = df.target\n",
    "print(target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE FOR RIGHT NOW\n",
    "# don't think we need to remove stop words with tf-idf but keep here\n",
    "\n",
    "#stop = set(stopwords.words('english'))\n",
    "\n",
    "# removes 'stop words' such as 'the', 'are', etc. it knows these stop words where i defined 'stop' variable, comes from a library\n",
    "#df['tweet_stop'] = df['lemmatized_tweet'].apply(lambda x: [y for y in x if y not in stop])\n",
    "\n",
    "# hashtags\n",
    "#df['hashtag'] = df.text.apply(lambda x: re.findall(r\"#(\\w+)\", x))\n",
    "\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE FOR RIGHT NOW\n",
    "\n",
    "#df.tweet_stop = df.tweet_stop.apply(lambda x: [' '.join(str(y)) for y in x])\n",
    "#print(df.tweet_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Tweet 1  Tweet 2  Tweet 3  Tweet 4  Tweet 5  Tweet 6  Tweet 7  \\\n",
      "aa            0        0        0        0        0        0        0   \n",
      "aahh          0        0        0        0        0        0        0   \n",
      "aal           0        0        0        0        0        0        0   \n",
      "aall          0        0        0        0        0        0        0   \n",
      "aamp          0        0        0        0        0        0        0   \n",
      "...         ...      ...      ...      ...      ...      ...      ...   \n",
      "zqp           0        0        0        0        0        0        0   \n",
      "zr            0        0        0        0        0        0        0   \n",
      "zumiez        0        0        0        0        0        0        0   \n",
      "zurich        0        0        0        0        0        0        0   \n",
      "zz            0        0        0        0        0        0        0   \n",
      "\n",
      "        Tweet 8  Tweet 9  Tweet 10  ...  Tweet 7605  Tweet 7606  Tweet 7607  \\\n",
      "aa            0        0         0  ...           0           0           0   \n",
      "aahh          0        0         0  ...           0           0           0   \n",
      "aal           0        0         0  ...           0           0           0   \n",
      "aall          0        0         0  ...           0           0           0   \n",
      "aamp          0        0         0  ...           0           0           0   \n",
      "...         ...      ...       ...  ...         ...         ...         ...   \n",
      "zqp           0        0         0  ...           0           0           0   \n",
      "zr            0        0         0  ...           0           0           0   \n",
      "zumiez        0        0         0  ...           0           0           0   \n",
      "zurich        0        0         0  ...           0           0           0   \n",
      "zz            0        0         0  ...           0           0           0   \n",
      "\n",
      "        Tweet 7608  Tweet 7609  Tweet 7610  Tweet 7611  Tweet 7612  \\\n",
      "aa               0           0           0           0           0   \n",
      "aahh             0           0           0           0           0   \n",
      "aal              0           0           0           0           0   \n",
      "aall             0           0           0           0           0   \n",
      "aamp             0           0           0           0           0   \n",
      "...            ...         ...         ...         ...         ...   \n",
      "zqp              0           0           0           0           0   \n",
      "zr               0           0           0           0           0   \n",
      "zumiez           0           0           0           0           0   \n",
      "zurich           0           0           0           0           0   \n",
      "zz               0           0           0           0           0   \n",
      "\n",
      "        Tweet 7613  frequency_summation  \n",
      "aa               0                    6  \n",
      "aahh             0                    1  \n",
      "aal              0                    1  \n",
      "aall             0                    1  \n",
      "aamp             0                    2  \n",
      "...            ...                  ...  \n",
      "zqp              0                    1  \n",
      "zr               0                    1  \n",
      "zumiez           0                    1  \n",
      "zurich           0                    1  \n",
      "zz               0                    1  \n",
      "\n",
      "[11573 rows x 7614 columns]\n"
     ]
    }
   ],
   "source": [
    "# IGNORE FOR RIGHT NOW - THIS IS THE COUNT VECTORIZER FOR IF WE WANT TO REMOVE KEYBOARD SMASHES LATER\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# remove terms that appear only once\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "term_frequencies = vectorizer.fit_transform(df.processed_tweet)\n",
    "\n",
    "# get vocabulary of terms\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "corpus_index = [f\"Tweet {i+1}\" for i in range(len(df.processed_tweet))]\n",
    "\n",
    "# create pandas DataFrame with term frequencies\n",
    "df_term_frequencies = pd.DataFrame(term_frequencies.T.todense(), index=feature_names, columns=corpus_index)\n",
    "\n",
    "df_term_frequencies['frequency_summation'] = df_term_frequencies.iloc[:].sum(axis=1)\n",
    "# df_term_frequencies = df_term_frequencies[df_term_frequencies['frequency_summation'] >= 2]\n",
    "print(df_term_frequencies.iloc[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aahh</th>\n",
       "      <th>aal</th>\n",
       "      <th>aall</th>\n",
       "      <th>aamp</th>\n",
       "      <th>aan</th>\n",
       "      <th>aand</th>\n",
       "      <th>aar</th>\n",
       "      <th>aashiqui</th>\n",
       "      <th>aat</th>\n",
       "      <th>...</th>\n",
       "      <th>zonesthank</th>\n",
       "      <th>zonewolf</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zotar</th>\n",
       "      <th>zouma</th>\n",
       "      <th>zqp</th>\n",
       "      <th>zr</th>\n",
       "      <th>zumiez</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 11573 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aa  aahh  aal  aall  aamp  aan  aand  aar  aashiqui  aat  ...  zonesthank  \\\n",
       "0  0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0       0.0  0.0  ...         0.0   \n",
       "1  0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0       0.0  0.0  ...         0.0   \n",
       "2  0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0       0.0  0.0  ...         0.0   \n",
       "3  0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0       0.0  0.0  ...         0.0   \n",
       "4  0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0       0.0  0.0  ...         0.0   \n",
       "\n",
       "   zonewolf  zoom  zotar  zouma  zqp   zr  zumiez  zurich   zz  \n",
       "0       0.0   0.0    0.0    0.0  0.0  0.0     0.0     0.0  0.0  \n",
       "1       0.0   0.0    0.0    0.0  0.0  0.0     0.0     0.0  0.0  \n",
       "2       0.0   0.0    0.0    0.0  0.0  0.0     0.0     0.0  0.0  \n",
       "3       0.0   0.0    0.0    0.0  0.0  0.0     0.0     0.0  0.0  \n",
       "4       0.0   0.0    0.0    0.0  0.0  0.0     0.0     0.0  0.0  \n",
       "\n",
       "[5 rows x 11573 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split up the matrix\n",
    "df_tf_idf.drop(df_tf_idf.columns[0], axis=1, inplace=True)\n",
    "features = df_tf_idf                      # feature matrix\n",
    "labels = target_df['target']              # target feature\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the features and the labels:\n",
      "\n",
      "train features:\n",
      "       aa  aahh  aal  aall  aamp  aan  aand  aar  aashiqui  aat  ...  \\\n",
      "5151  0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0       0.0  0.0  ...   \n",
      "6351  0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0       0.0  0.0  ...   \n",
      "3443  0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0       0.0  0.0  ...   \n",
      "7164  0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0       0.0  0.0  ...   \n",
      "7037  0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0       0.0  0.0  ...   \n",
      "...   ...   ...  ...   ...   ...  ...   ...  ...       ...  ...  ...   \n",
      "5226  0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0       0.0  0.0  ...   \n",
      "5390  0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0       0.0  0.0  ...   \n",
      "860   0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0       0.0  0.0  ...   \n",
      "7603  0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0       0.0  0.0  ...   \n",
      "7270  0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0       0.0  0.0  ...   \n",
      "\n",
      "      zonesthank  zonewolf  zoom  zotar  zouma  zqp   zr  zumiez  zurich   zz  \n",
      "5151         0.0       0.0   0.0    0.0    0.0  0.0  0.0     0.0     0.0  0.0  \n",
      "6351         0.0       0.0   0.0    0.0    0.0  0.0  0.0     0.0     0.0  0.0  \n",
      "3443         0.0       0.0   0.0    0.0    0.0  0.0  0.0     0.0     0.0  0.0  \n",
      "7164         0.0       0.0   0.0    0.0    0.0  0.0  0.0     0.0     0.0  0.0  \n",
      "7037         0.0       0.0   0.0    0.0    0.0  0.0  0.0     0.0     0.0  0.0  \n",
      "...          ...       ...   ...    ...    ...  ...  ...     ...     ...  ...  \n",
      "5226         0.0       0.0   0.0    0.0    0.0  0.0  0.0     0.0     0.0  0.0  \n",
      "5390         0.0       0.0   0.0    0.0    0.0  0.0  0.0     0.0     0.0  0.0  \n",
      "860          0.0       0.0   0.0    0.0    0.0  0.0  0.0     0.0     0.0  0.0  \n",
      "7603         0.0       0.0   0.0    0.0    0.0  0.0  0.0     0.0     0.0  0.0  \n",
      "7270         0.0       0.0   0.0    0.0    0.0  0.0  0.0     0.0     0.0  0.0  \n",
      "\n",
      "[5709 rows x 11573 columns]\n",
      "test features:\n",
      "       aa  aahh  aal  aall  aamp  aan  aand  aar  aashiqui  aat  ...  \\\n",
      "2644  0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0       0.0  0.0  ...   \n",
      "2227  0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0       0.0  0.0  ...   \n",
      "5448  0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0       0.0  0.0  ...   \n",
      "132   0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0       0.0  0.0  ...   \n",
      "6845  0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0       0.0  0.0  ...   \n",
      "...   ...   ...  ...   ...   ...  ...   ...  ...       ...  ...  ...   \n",
      "5209  0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0       0.0  0.0  ...   \n",
      "387   0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0       0.0  0.0  ...   \n",
      "4848  0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0       0.0  0.0  ...   \n",
      "1032  0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0       0.0  0.0  ...   \n",
      "7195  0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0       0.0  0.0  ...   \n",
      "\n",
      "      zonesthank  zonewolf  zoom  zotar  zouma  zqp   zr  zumiez  zurich   zz  \n",
      "2644         0.0       0.0   0.0    0.0    0.0  0.0  0.0     0.0     0.0  0.0  \n",
      "2227         0.0       0.0   0.0    0.0    0.0  0.0  0.0     0.0     0.0  0.0  \n",
      "5448         0.0       0.0   0.0    0.0    0.0  0.0  0.0     0.0     0.0  0.0  \n",
      "132          0.0       0.0   0.0    0.0    0.0  0.0  0.0     0.0     0.0  0.0  \n",
      "6845         0.0       0.0   0.0    0.0    0.0  0.0  0.0     0.0     0.0  0.0  \n",
      "...          ...       ...   ...    ...    ...  ...  ...     ...     ...  ...  \n",
      "5209         0.0       0.0   0.0    0.0    0.0  0.0  0.0     0.0     0.0  0.0  \n",
      "387          0.0       0.0   0.0    0.0    0.0  0.0  0.0     0.0     0.0  0.0  \n",
      "4848         0.0       0.0   0.0    0.0    0.0  0.0  0.0     0.0     0.0  0.0  \n",
      "1032         0.0       0.0   0.0    0.0    0.0  0.0  0.0     0.0     0.0  0.0  \n",
      "7195         0.0       0.0   0.0    0.0    0.0  0.0  0.0     0.0     0.0  0.0  \n",
      "\n",
      "[1904 rows x 11573 columns]\n",
      "train labels:\n",
      "5151    0\n",
      "6351    0\n",
      "3443    0\n",
      "7164    1\n",
      "7037    1\n",
      "       ..\n",
      "5226    0\n",
      "5390    0\n",
      "860     0\n",
      "7603    1\n",
      "7270    1\n",
      "Name: target, Length: 5709, dtype: int64\n",
      "test labels:\n",
      "2644    1\n",
      "2227    0\n",
      "5448    1\n",
      "132     0\n",
      "6845    0\n",
      "       ..\n",
      "5209    0\n",
      "387     1\n",
      "4848    1\n",
      "1032    0\n",
      "7195    1\n",
      "Name: target, Length: 1904, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5709, 11573)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)\n",
    "\n",
    "print('These are the features and the labels:\\n')\n",
    "print('train features:')\n",
    "print(train_features)\n",
    "print('test features:')\n",
    "print(test_features)\n",
    "print('train labels:')\n",
    "print(train_labels)\n",
    "print('test labels:')\n",
    "print(test_labels)\n",
    "\n",
    "#only to see if filters later work\n",
    "train_features.shape\n",
    "#test_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Wrapper method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Wrapper method 1 with k range - selma\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "sfs = SFS(RandomForestClassifier(), \n",
    "           k_features=(3, 15),\n",
    "           forward=True, \n",
    "           floating=False, \n",
    "           scoring='accuracy',\n",
    "           cv=5)\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), sfs)\n",
    "\n",
    "pipe.fit(train_features, train_labels)\n",
    "\n",
    "print('best combination (ACC: %.3f): %s\\n' % (sfs.k_score_, sfs.k_feature_idx_))\n",
    "sfs.k_feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Wrapper method 2 - selma\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "sfs = SFS(RandomForestClassifier(),\n",
    "          k_features=100,\n",
    "          forward=True,\n",
    "          floating=False,\n",
    "          scoring = 'accuracy',\n",
    "          cv = 0)\n",
    "\n",
    "sfs.fit(train_features, train_labels)\n",
    "\n",
    "#print('best combination (ACC: %.3f): %s\\n' % (sfs.k_score_, sfs.k_feature_idx_))\n",
    "print('best combination (ACC: %.3f)' % (sfs.k_score_))\n",
    "#sfs.k_feature_names_     # to get the final set of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aand', 'aashiqui', 'aayf', 'abes', 'abeyth', 'aboard', 'absence', 'accidentalprophecy', 'accidentwho', 'accuracy', 'acquiesce', 'actavis', 'actin', 'acura', 'adamantly', 'addiction', 'adjuster', 'adoptive', 'aeg', 'affic', 'afloat', 'aggarwal', 'agnivesh', 'agt', 'aguero', 'ahahahga', 'ahamedis', 'ahrar', 'aii', 'airbullet', 'airhead', 'airhorns', 'airlift', 'akilah', 'akrams', 'akwa', 'alaskaseafood', 'albertsons', 'alchemist', 'alexandrian', 'alexis', 'ali', 'alice', 'allied', 'alloosh', 'almighty', 'alois', 'alternate', 'aluminum', 'alwx', 'ambition', 'ambleside', 'ameribag', 'amicos', 'amicospizzato', 'amiddleaged', 'ampamp', 'ampask', 'ampstart', 'ampwanted', 'amreading', 'amritsar', 'anakin', 'anders', 'ani', 'animation', 'anime', 'annonymous', 'anonymous', 'anthology', 'antifeminist', 'antiochus', 'anu', 'anxietyproblems', 'anytime', 'apiece', 'appropriate', 'appropriation', 'april', 'archetype', 'areal', 'aredeluged', 'argsuppose', 'arian', 'ariana', 'ariz', 'arra', 'arrange', 'arse', 'arti', 'ary', 'ashley', 'ashville', 'askforalaska', 'askin', 'aspire', 'assailant', 'assassin', 'assault', 'assistant', 'astrakhan', 'astrologian', 'atamathon', 'atc', 'atom', 'atomicbomb', 'aurora', 'autism', 'autistic', 'autoinsurance', 'avenge', 'aviationaddicts', 'avigdor', 'awaken', 'awareness', 'awesomesauce', 'awn', 'ayy', 'az', 'baack', 'bachelorette', 'backlash', 'backpack', 'backto', 'backty', 'badchoices', 'badkitty', 'badly', 'baekhyun', 'bahrain', 'bait', 'bamp', 'bangampmy', 'bangladeshaffected', 'bantrophyhunting', 'bard', 'basalt', 'bask', 'bathroomits', 'bcs', 'bcz', 'beastin', 'beatz', 'becyme', 'beep', 'beheading', 'beige', 'beinart', 'belie', 'believeis', 'belligerent', 'belowthe', 'bengston', 'benjamin', 'benny', 'benothing', 'benstracy', 'bentley', 'berahino', 'bestival', 'bestseller', 'beth', 'bias', 'biblical', 'bicentennial', 'bickleton', 'bigconsequences', 'biggar', 'bigstar', 'bio', 'biryani', 'bitchruns', 'blaa', 'blacken', 'blackmail', 'bleacher', 'blend', 'bliss', 'blogger', 'bloor', 'blowin', 'blowltan', 'bluff', 'bmx', 'boise', 'bolasie', 'bombedout', 'bong', 'bonsai', 'boo', 'bookslast', 'boost', 'boulder', 'boutta', 'bow', 'bowhunting', 'boycott', 'boyhaus', 'brazilian', 'brb', 'breast', 'breath', 'breivik', 'bri', 'bridal', 'briefcase', 'brighton', 'brilliant', 'brisbane', 'bromley', 'brunt', 'bst', 'bstop', 'bud', 'buddz', 'buffetts', 'builder', 'bukidnon', 'bull', 'bumpin', 'burger', 'burnfat', 'busines', 'buyout', 'bwp', 'cafeawful', 'calais', 'calmly', 'cameo', 'candidate', 'cannabis', 'canon', 'cantar', 'cantwont', 'capable', 'cape', 'capricon', 'captor', 'cara', 'caravan', 'carbondale', 'careerbest', 'carhot', 'carliles', 'carmi', 'carpooling', 'carriage', 'caseit', 'casing', 'catcher', 'catholicismhomophobia', 'causation', 'cbcto', 'ccot', 'cedar', 'cereal', 'cern', 'certainty', 'cgi', 'chachi', 'champagne', 'champoo', 'chandanee', 'chapoutier', 'chapter', 'charlize', 'charminar', 'charon', 'chaser', 'chawal', 'che', 'checkedhe', 'cheryl', 'chiang', 'chiasson', 'chili', 'chilli', 'chim', 'chinadotcom', 'choppas', 'chopras', 'chorbjp', 'chuck', 'chucker', 'cigs', 'citation', 'cityampothers', 'cityof', 'civility', 'clain', 'clara', 'clay', 'cle', 'cleav', 'cleavage', 'clelliyou', 'clerical', 'climatechange', 'cliptv', 'clipuri', 'closely', 'clot', 'clueless', 'clyde', 'cnmi', 'cocker', 'cocoa', 'cod', 'coefficient', 'cog', 'coldblooded', 'collectible', 'collie', 'colombia', 'colomr', 'colonel', 'colorism', 'commencement', 'compare', 'compassion', 'compel', 'conditionsprivation', 'conf', 'confound', 'connecticut', 'consciousness', 'conserve', 'constellation', 'constitutional', 'consumerist', 'container', 'contd', 'contemplation', 'contemporary', 'contribute', 'convection', 'convention', 'convertible', 'convince', 'convivted', 'coop', 'cooperation', 'coronet', 'correlation', 'cosima', 'costar', 'coulda', 'coun', 'countdown', 'counterstrike', 'courageous', 'courtof', 'cowardly', 'coworkers', 'cowx', 'coyi', 'crapgamer', 'crashsterling', 'crave', 'creepiest', 'crest', 'critically', 'criticalmedia', 'critter', 'crossbody', 'crossfit', 'crozes', 'crucial', 'crunchy', 'csismica', 'cu', 'cube', 'cullen', 'cultural', 'cutlery', 'cutter', 'cuyahoga', 'cyber', 'cyhi', 'dadwho', 'daem', 'dagestan', 'dajaal', 'dak', 'damascus', 'dampen', 'davis', 'dawabsha', 'dawgs', 'dawson', 'daytonarea', 'dazzle', 'ddnt', 'deadpool', 'deathholy', 'decay', 'declarat', 'decrease', 'decriminalize', 'deem', 'deepak', 'deepthoughts', 'defect', 'defensive', 'deglin', 'delude', 'demo', 'demonstrate', 'descend', 'desolationofsmaug', 'despicable', 'dessicated', 'destructive', 'detention', 'detonateamp', 'deviate', 'dew', 'dewdney', 'df', 'dhaka', 'dialysis', 'diaporama', 'dicky', 'dierks', 'dilutional', 'dip', 'directory', 'disciplinary', 'discourse', 'disdain', 'disembarkment', 'disinfo', 'disregard', 'distinction', 'diversify', 'dmpl', 'domain', 'dontexpectnothing', 'donthate', 'donut', 'doom', 'doone', 'dopest', 'doppler', 'doretts', 'doris', 'doritos', 'dose', 'dot', 'dotish', 'dougs', 'dove', 'downtime', 'draft', 'dramatically', 'dribble', 'driller', 'driverless', 'driverlesscars', 'drumstep', 'dtb', 'dua', 'dubbo', 'dupree', 'dv', 'dy', 'dye', 'dynamix', 'earnest', 'ecstatic', 'eddy', 'edge', 'edmond', 'edna', 'edwing', 'efs', 'egyptian', 'elanorofrohan', 'electrocutedboiling', 'elk', 'ella', 'elvia', 'elwoods', 'embarrassment', 'emmanuel', 'emp', 'empathy', 'empower', 'emv', 'encourage', 'enkelbiljett', 'ent', 'entertain', 'entertainer', 'entirely', 'entrance', 'eo', 'eopedia', 'epiphanes', 'equation', 'erdogans', 'erode', 'ese', 'esoteric', 'esse', 'ethic', 'ethical', 'ethiopia', 'eto', 'eudry', 'eulogy', 'eurocrisis', 'euroquake', 'eurostyle', 'evac', 'everythign', 'evildead', 'ew', 'ewsdesk', 'exaggerate', 'excessive', 'exotic', 'expansion', 'expertwhiner', 'explosivespacked', 'explosivesrigged', 'export', 'expressioncheeks', 'extent', 'externally', 'ey', 'eyeball', 'ezidi', 'fab', 'faceless', 'facemarvins', 'fadden', 'faint', 'famp', 'fanfiction', 'fantabulous', 'fantsticwhatever', 'farmr', 'fastfurious', 'fatburning', 'fault', 'faved', 'favori', 'favoritism', 'favourite', 'fcked', 'fdny', 'fecal', 'ferrochrome', 'fest', 'fiat', 'fiery', 'fighterdena', 'financially', 'finna', 'firearm', 'firmly', 'fjord', 'flurry', 'fml', 'fna', 'foam', 'foamcc', 'foe', 'followup', 'forage', 'forbesasia', 'forbid', 'forney', 'forsee', 'fotoset', 'foursquare', 'foxnews', 'fp', 'franta', 'freezerdummy', 'friendly', 'frontin', 'ftsn', 'fu', 'fuckboy', 'fucktard', 'fujiwara', 'fulfil', 'fundwhen', 'funniest', 'funtimes', 'fwt', 'fxa', 'gaia', 'galactics', 'gambia', 'gambit', 'gamefeed', 'gamescom', 'gangstermail', 'garage', 'gazebo', 'genitals', 'gentlementhe', 'georgina', 'ghetto', 'gio', 'girardeau', 'giselle', 'gishwhes', 'gist', 'glade', 'glimpse', 'globiinclusion', 'gly', 'gog', 'goggles', 'gohan', 'goin', 'golem', 'gooaal', 'gotham', 'gotthard', 'goz', 'grandmother', 'graph', 'graveyard', 'greening', 'greenway', 'greer', 'greet', 'greystone', 'griffin', 'grime', 'gritty', 'growingup', 'guage', 'guam', 'guatemala', 'guatemalan', 'gucci', 'guevara', 'gujarat', 'gymtime', 'halfderailed', 'halfhr', 'halfrailed', 'halfway', 'hamming', 'hammond', 'hamper', 'hamstring', 'handside', 'hangin', 'hanover', 'harda', 'hardcover', 'hardly', 'hardside', 'harlan', 'harman', 'harmony', 'harris', 'harvest', 'has', 'hashimoto', 'hater', 'hawk', 'hazardu', 'hcg', 'headquarters', 'healthinsurance', 'healthy', 'heartache', 'hed', 'helios', 'heller', 'helpme', 'helsinki', 'hempoil', 'hendricks', 'hendrixonfire', 'henshaw', 'heredeluge', 'hermitage', 'hew', 'hicksville', 'higuains', 'hillymountain', 'hindustan', 'hirochii', 'hist', 'hm', 'hogan', 'hoke', 'holly', 'hollyw', 'holmes', 'homegrown', 'homey', 'hond', 'honshu', 'hopfer', 'hospitality', 'howell', 'hse', 'hta', 'html', 'htt', 'http', 'hubris', 'hull', 'hustle', 'hyderabad', 'ib', 'ibom', 'icebreaker', 'iceland', 'icelandreview', 'idaho', 'idm', 'idwx', 'iee', 'ignoranceshe', 'ihate', 'illegality', 'illuminate', 'imaging', 'imagini', 'imdb', 'imminent', 'imo', 'imouto', 'implode', 'implore', 'imply', 'incessant', 'incinerator', 'incitement', 'ind', 'indiana', 'indianperpetrated', 'indiefilm', 'individl', 'indonesian', 'indot', 'ineedexposure', 'infact', 'infinite', 'influential', 'infrastructure', 'innit', 'input', 'insect', 'insert', 'insight', 'insomniac', 'inspiration', 'instagramers', 'institutionalize', 'insure', 'intense', 'interaction', 'intersperse', 'intl', 'intro', 'invent', 'investigative', 'investment', 'io', 'iot', 'ipo', 'irandeal', 'iraqi', 'isaiah', 'isao', 'isinjury', 'ist', 'itblank', 'itd', 'itinerary', 'jacquie', 'jag', 'jagm', 'jak', 'jamaican', 'jeep', 'jeepsunk', 'jeez', 'jeffersonamp', 'jetalerts', 'jihadis', 'jimmy', 'joey', 'jogger', 'jokin', 'josephus', 'josie', 'jota', 'jsdf', 'juan', 'judith', 'jumpin', 'junction', 'juneau', 'junk', 'justsaying', 'jwalk', 'kaboom', 'kabul', 'kabwandi', 'kalmikya', 'kamloops', 'kane', 'karlsruhe', 'kc', 'keepingtheviginaclean', 'kellogg', 'kenosha', 'keratin', 'keurig', 'kevin', 'kfvs', 'khaki', 'kiddos', 'kidsthesedays', 'killsone', 'kissimmee', 'kml', 'knight', 'knott', 'knox', 'kosher', 'krefeld', 'kung', 'kunstler', 'kurtkamka', 'lace', 'lafayette', 'lag', 'lagos', 'laighign', 'lakh', 'lamb', 'landowner', 'langtree', 'lantiqua', 'lapaka', 'lastingness', 'lastma', 'latinoand', 'lattice', 'lawrence', 'lawx', 'lbr', 'leaf', 'lean', 'leezy', 'leftwich', 'lefty', 'legwalked', 'lembra', 'lessonforlife', 'lest', 'levy', 'lhmovie', 'liberal', 'liberman', 'libs', 'lickin', 'lifeits', 'lifestyle', 'lightman', 'lightniike', 'lindenow', 'lipstick', 'lithium', 'liverpool', 'livery', 'livin', 'ljev', 'lk', 'locate', 'lockdown', 'locust', 'loko', 'longll', 'looney', 'louderthings', 'lougheed', 'ltmeltdown', 'ltz', 'lujo', 'luka', 'lulu', 'lyf', 'lyme', 'lyndon', 'maailiss', 'maan', 'maca', 'macia', 'macon', 'magical', 'magog', 'magu', 'maiga', 'maine', 'mainland', 'makinmemories', 'manarmed', 'manchesterlite', 'mandatory', 'mane', 'manga', 'manifestation', 'mankind', 'manor', 'manservant', 'mantra', 'manual', 'manually', 'manuel', 'manure', 'mapleridge', 'marin', 'marinelines', 'marketing', 'marlene', 'mart', 'marvellous', 'marxism', 'mason', 'massage', 'masse', 'masterfsloths', 'mataas', 'matakomilk', 'matias', 'mattress', 'mav', 'mayonnaise', 'mayor', 'mcgsecure', 'mcpherson', 'meade', 'meaningless', 'measure', 'meddling', 'medinah', 'mediocrity', 'medusa', 'meg', 'megalpolis', 'mel', 'memorable', 'menahem', 'mencius', 'menolippu', 'meow', 'mercenary', 'mere', 'mesick', 'messiah', 'metepec', 'metrofmtalk', 'mfi', 'mfrwauthor', 'mfsloose', 'mgtab', 'mht', 'michel', 'mics', 'milita', 'milledgeville', 'milwaukee', 'minutia', 'miracle', 'mirkwood', 'missile', 'missouri', 'misstep', 'mixtape', 'mixxtail', 'mlm', 'mmda', 'mn', 'moblins', 'modestmouseremix', 'modibo', 'molave', 'mole', 'molecularly', 'molloy', 'molys', 'mommy', 'monologue', 'montana', 'moonbeam', 'moonlight', 'mopheme', 'morebut', 'moregt', 'morel', 'morningamp', 'mortar', 'moshav', 'motorist', 'movingk', 'movt', 'mthompson', 'mtr', 'muaytai', 'multidimensi', 'mumbailocals', 'mumbaix', 'murphy', 'muse', 'mushroom', 'muslimsterrorism', 'mustard', 'muster', 'mute', 'mutilate', 'mydrought', 'mystical', 'm¼', 'naemolgo', 'nailreal', 'naked', 'nan', 'nanking', 'nanotech', 'naomi', 'nap', 'nar', 'narcissism', 'narrative', 'nars', 'natalie', 'natwest', 'nbanews', 'nbcni', 'nbn', 'neely', 'neglect', 'negligence', 'negotiation', 'nephew', 'nervous', 'nevermore', 'nflexpertpicks', 'nieuws', 'nigh', 'nightmarish', 'nikoniko', 'nineyearold', 'nino', 'nit', 'nixon', 'nkea', 'nlwx', 'noah', 'noi', 'nonexistant', 'noob', 'nook', 'noranda', 'northgate', 'northumberland', 'notion', 'notoriety', 'nova', 'nowplay', 'npi', 'numbers', 'nuu', 'nwt', 'nyg', 'nypd', 'oamsgajagahahah', 'observation', 'observer', 'obstacle', 'ocalan', 'occupation', 'ocd', 'oddball', 'offense', 'offshore', 'offside', 'oficial', 'ofr', 'ogt', 'oj', 'onduty', 'ongoing', 'onofre', 'onscene', 'ood', 'ophelia', 'optimistic', 'organ', 'organism', 'orianna', 'orphanage', 'orshow', 'osha', 'ossington', 'ostentatious', 'otherwisethey', 'ouch', 'overall', 'overblown', 'overboard', 'overflow', 'overrun', 'oversight', 'overwork', 'ow', 'owenrbroadhurst', 'ownership', 'oworoshoki', 'ox', 'paeds', 'painthey', 'pale', 'paleface', 'pamper', 'pantofel', 'papii', 'paramore', 'parch', 'particular', 'passion', 'pastor', 'pasture', 'pat', 'patron', 'paypile', 'peep', 'pen', 'penetrate', 'pension', 'perforate', 'permission', 'pertain', 'perth', 'pestle', 'petebests', 'petersens', 'pg', 'pharma', 'phenomenal', 'phew', 'philip', 'phillip', 'phuket', 'physically', 'piano', 'pickpocket', 'pictwittercomp', 'pierc', 'pigeon', 'pileq', 'pillory', 'pioneer', 'plank', 'planner', 'plantation', 'playa', 'plea', 'plead', 'pletch', 'pluto', 'plz', 'pmclose', 'pneumonia', 'poc', 'poison', 'politic', 'pollute', 'pone', 'ponting', 'popeyes', 'populardemand', 'porridge', 'portrait', 'possesion', 'possess', 'pouch', 'poway', 'poze', 'ppact', 'pple', 'prabhu', 'practically', 'preborn', 'precedent', 'preseason', 'preseasonworkouts', 'prevalent', 'printable', 'prisonlike', 'privilege', 'proc', 'profithungry', 'promo', 'promotion', 'propane', 'propertycasu', 'propertycasualty', 'proportion', 'provider', 'psd', 'pu', 'publication', 'publicityalthough', 'pugwash', 'puledotechupdate', 'pulkovo', 'pumpkin', 'punishable', 'punishment', 'punk', 'puppet', 'purify', 'pwz', 'qendil', 'qin', 'quantum', 'quarterstaff', 'queer', 'quem', 'quicker', 'quit', 'racco', 'radley', 'radneck', 'raffi', 'raft', 'randomthought', 'ratingbut', 'rc', 'rcityporn', 'rconspiracy', 'rdconsider', 'rdo', 'reaction', 'reader', 'readiness', 'reagan', 'rebound', 'recluse', 'reconnect', 'redblood', 'redeemer', 'rediscover', 'redlands', 'redo', 'redwing', 'reference', 'regr', 'regress', 'reidlake', 'relevance', 'religious', 'renewsit', 'reportly', 'represent', 'reqd', 'requa', 'rescind', 'residualincome', 'resort', 'respondent', 'retreat', 'reunite', 'reversal', 'revise', 'rgj', 'rhyme', 'ridah', 'rigour', 'riser', 'rlyeh', 'roadworks', 'roanoke', 'robertwelch', 'robin', 'roblox', 'roddy', 'rom', 'romantic', 'romanticsuspense', 'romp', 'ronald', 'roofing', 'rosenbergs', 'rosenthalauthor', 'rotary', 'rotator', 'routine', 'rpics', 'rsx', 'ru', 'rub', 'rumble', 'rumor', 'runabout', 'runner', 'rvr', 'sabotage', 'saddledome', 'safeco', 'safer', 'safsufa', 'saku', 'salopek', 'samsung', 'sanchez', 'sandbox', 'sandwich', 'sanford', 'sanitise', 'santiago', 'saturated', 'savage', 'savs', 'scanner', 'scenario', 'schulz', 'scoopit', 'scotiabank', 'seamstress', 'sec', 'sector', 'seemly', 'seeyouatamicos', 'segment', 'sensei', 'senzu', 'septic', 'ser', 'sevenfold', 'sever', 'sexist', 'sfgate', 'sfor', 'shad', 'shaolin', 'shay', 'shear', 'shen', 'sherfield', 'shestooyoung', 'shirley', 'shiver', 'shook', 'shortfall', 'shrew', 'shrewsbury', 'shunichiro', 'sica', 'sigalert', 'sigh', 'signatureschange', 'signin', 'silverwood', 'singalong', 'sioux', 'siouxlan', 'siouxland', 'sip', 'siteinvestigating', 'situ', 'sivan', 'sixcar', 'sixpencee', 'siz', 'sj', 'skateboard', 'skc', 'skeleton', 'skylanders', 'skype', 'skywarn', 'slander', 'slashandburn', 'slate', 'slew', 'slingnews', 'slowpoke', 'slowreport', 'slum', 'smack', 'smem', 'smoker', 'smokey', 'smoochy', 'smp', 'sneezing', 'snipe', 'snoop', 'snowstormdespite', 'snowy', 'soaker', 'soap', 'socialist', 'sock', 'soil', 'somethingyr', 'somewhere', 'sooth', 'sophistication', 'sour', 'southaccident', 'soz', 'sparkz', 'speccy', 'specialguest', 'specialneeds', 'spectrum', 'speech', 'spen', 'splifs', 'spokane', 'spongea', 'sprite', 'srk', 'ssp', 'stable', 'stacy', 'stagetwo', 'stall', 'statistically', 'stavola', 'stearns', 'stefano', 'sterotypical', 'steven', 'stm', 'stopevictions', 'storey', 'stowing', 'straighten', 'stratford', 'stree', 'streetjamz', 'strength', 'stripe', 'stud', 'subatomic', 'subcommittee', 'submitt', 'substance', 'subtlety', 'succeed', 'sudan', 'sugar', 'suho', 'sultry', 'summerfate', 'sumo', 'sunburn', 'sundaydont', 'sunnymeade', 'superficial', 'superfood', 'superman', 'supposedly', 'supremacist', 'suresh', 'susiya', 'sustain', 'sustainability', 'sutherland', 'swag', 'swami', 'sweaty', 'swoop', 'syjexo', 'tacoma', 'tactical', 'tafs', 'takeoff', 'tall', 'tambo', 'tampon', 'tan', 'tanto', 'tapa', 'tarzana', 'taxi', 'technews', 'technica', 'teenager', 'telangana', 'teleport', 'tellyamp', 'temp', 'template', 'tenn', 'testa', 'texan', 'tfl', 'tgirl', 'theaf', 'thehobbit', 'themthen', 'thi', 'thief', 'thirtyfive', 'thorins', 'thranduil', 'thugging', 'tight', 'tinder', 'tindering', 'tint', 'tiny', 'tj', 'tmr', 'toke', 'tolerance', 'tomislav', 'tone', 'topstories', 'torment', 'torrance', 'torrecilla', 'totoo', 'tottenham', 'touchdown', 'tp', 'tracy', 'trafficnetwork', 'trailhead', 'trancy', 'transcend', 'transgender', 'translate', 'tray', 'treaty', 'trespass', 'trey', 'triangle', 'trickshot', 'tricky', 'tricycle', 'trixiedrowned', 'truckload', 'truestory', 'trusty', 'tstorm', 'turdnado', 'tusky', 'twain', 'twenty', 'twi', 'twilight', 'twitch', 'udom', 'ufn', 'uke', 'uluru', 'umntu', 'unauthorized', 'uncertaintyeconomic', 'uncomfortable', 'undermine', 'underpass', 'undetected', 'unending', 'unfair', 'unhealed', 'unimaginable', 'unlicensed', 'unpack', 'unprepared', 'unqualified', 'unscreened', 'unsurprisingly', 'unu', 'unwanted', 'upload', 'upstairs', 'uptotheminute', 'upward', 'upwards', 'urge', 'urgentthere', 'urself', 'uruan', 'usagi', 'useless', 'usg', 'usw', 'uud', 'uvo', 'uw', 'ux', 'vallerand', 'valleywx', 'vanuatu', 'vassalboro', 'vector', 'veg', 'venom', 'verge', 'versus', 'vessel', 'vickers', 'videoclip', 'viking', 'ville', 'violate', 'violet', 'violin', 'viper', 'visage', 'visting', 'vita', 'vmas', 'volcanic', 'volcanoin', 'volgagrad', 'volunteer', 'voortrekker', 'vortex', 'vu', 'vulcan', 'vulnerability', 'vuzu', 'wackoes', 'wakho', 'wanother', 'wantmyabsback', 'warfighting', 'warmth', 'warne', 'warner', 'warrant', 'wartime', 'washable', 'wasnampt', 'wasp', 'watchin', 'watchout', 'waterboarding', 'waterfur', 'watermeloann', 'watermelon', 'waterproof', 'watersafety', 'watersland', 'wayward', 'wbc', 'wbre', 'wdtv', 'weatherit', 'wedgie', 'wei', 'weightless', 'wellgrounded', 'wen', 'wereonadesolateplanet', 'westminister', 'wew', 'wexler', 'wf', 'wfp', 'wharf', 'whatcanthedo', 'whatsapp', 'wheat', 'whensoever', 'whimsy', 'whiskey', 'whitehouse', 'wiedemer', 'wikipedia', 'wildlife', 'wildlooking', 'windmy', 'windowsill', 'wld', 'wnp', 'woah', 'woodchuck', 'wooden', 'wording', 'workspace', 'worldpay', 'worsen', 'worstsummerjob', 'worthless', 'wots', 'wowthe', 'wpd', 'wpo', 'wrestler', 'wrinkle', 'wrongway', 'wroug', 'wsj', 'wsvr', 'wyou', 'xavier', 'yard', 'yazidi', 'yazidishingalgenocide', 'ydu', 'yeat', 'yelp', 'yessum', 'ygw', 'yield', 'yiraneuni', 'yuan', 'yum', 'yumiko', 'yuvi', 'yx', 'yzf', 'zaatari', 'zabadani', 'zergele', 'zi', 'zicac', 'zimmer', 'zimmerman', 'zlore', 'zo', 'zodiac', 'zonesthank', 'zoom', 'zotar', 'zr', 'zurich']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5709, 9832)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing constant features\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "#Create constant filter and apply to training features\n",
    "constant_filter = VarianceThreshold(threshold=0)\n",
    "constant_filter.fit(train_features)\n",
    "\n",
    "len(train_features.columns[constant_filter.get_support()])\n",
    "\n",
    "constant_columns = [column for column in train_features.columns\n",
    "                    if column not in train_features.columns[constant_filter.get_support()]]\n",
    "print(constant_columns)\n",
    "\n",
    "train_features.drop(labels=constant_columns, axis=1, inplace=True)\n",
    "test_features.drop(labels=constant_columns, axis=1, inplace=True)\n",
    "\n",
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5709, 9832)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing Quasi-constant features\n",
    "# Define the threshold as 0.01 and create the quasi constant filter\n",
    "q_constant_remover = VarianceThreshold(threshold=0.01)\n",
    "\n",
    "#Apply filter to the training set\n",
    "q_constant_remover.fit(train_features) \n",
    "\n",
    "#See that the training set now only contains non-constant and non-quasi features\n",
    "train_features = q_constant_remover.transform(train_features)\n",
    "test_features = q_constant_remover.transform(test_features)\n",
    "\n",
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing duplicates has been removed as it already should be done with tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5709, 6998)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#REMOVE CORRELATED FEATURES\n",
    "train_features= pd.DataFrame(train_features)\n",
    "test_features= pd.DataFrame(test_features)\n",
    "\n",
    "correlated_features = set()\n",
    "correlation_matrix = train_features.corr()\n",
    "    \n",
    "for i in range(len(correlation_matrix .columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.9:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.add(colname)\n",
    "            \n",
    "train_features.drop(labels=correlated_features, axis=1, inplace=True)\n",
    "test_features.drop(labels=correlated_features, axis=1, inplace=True)\n",
    "\n",
    "train_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR WRAPPER METHOD - GENERATE THE NEW TRAIN AND TEST DATAFRAMES BASED ON SELECTED FEATURES\n",
    "\n",
    "# Note that the transform call is equivalent to\n",
    "# features_train[:, sfs.k_feature_idx_]\n",
    "\n",
    "features_train_sfs = sfs.transform(train_features)\n",
    "features_test_sfs = sfs.transform(test_features)\n",
    "print(features_train_sfs)\n",
    "print(features_test_sfs)\n",
    "\n",
    "# Fit the estimator using the new feature subset\n",
    "# and make a prediction on the test data\n",
    "model = RandomForestClassifier()\n",
    "model.fit(features_train_sfs, train_labels)\n",
    "labels_pred = model.predict(features_test_sfs)\n",
    "\n",
    "# Compute the accuracy of the prediction\n",
    "acc = float((test_labels == labels_pred).sum()) / labels_pred.shape[0]\n",
    "print('Test set accuracy: %.2f %%' % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is RandomForest score:\n",
      "0.7841386554621849\n"
     ]
    }
   ],
   "source": [
    "# RandomForest - train the model and test, report score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "print('\\nThis is RandomForest score:')\n",
    "model = RandomForestClassifier()\n",
    "model.fit(train_features, train_labels)\n",
    "print(model.score(test_features, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression - train the model and test, report score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "print('\\nThis is Logistic Regression score:')\n",
    "model2 = LogisticRegression()\n",
    "model2.fit(train_features, train_labels)\n",
    "model2.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
