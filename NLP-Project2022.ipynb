{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Machine Learning Project 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links:\n",
    "\n",
    "https://towardsdatascience.com/natural-language-processing-nlp-for-machine-learning-d44498845d5b\n",
    "https://www.andyfitzgeraldconsulting.com/writing/keyword-extraction-nlp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import re\n",
    "from nltk.probability import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "# from term_frequency import term_frequencies, feature_names, df_term_frequencies\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "transformer = TfidfTransformer()\n",
    "tt = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'train.csv')\n",
    "\n",
    "df.replace('NaN', np.NaN, inplace = True)\n",
    "\n",
    "# to count the number of NaN's in each column, just change the column name in this line to see how many missing values of that\n",
    "# variable per other column\n",
    "# print(df[df.keyword.isnull()].count())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tweet_noNoise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                      tweet_noNoise  \n",
       "0       1  our deeds are the reason of this earthquake ma...  \n",
       "1       1              forest fire near la ronge sask canada  \n",
       "2       1  all residents asked to shelter in place are be...  \n",
       "3       1  13000 people receive wildfires evacuation orde...  \n",
       "4       1  just got sent this photo from ruby alaska as s...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_tweets(text):\n",
    "    \n",
    "# remove mentions and URLs\n",
    "    text_noMentionURL = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", text)\n",
    "    text_noMentionURL = \" \".join(text_noMentionURL.split())\n",
    "    \n",
    "# remove '#' symbols and add space before capital letter\n",
    "    text_noHash = re.sub(r\"([A-Z]+)\", r\" \\1\", text)\n",
    "    text_noHash = re.sub(r\"(#)\", \"\", text_noHash)\n",
    "    text_noHash = \" \".join(text_noHash.split())\n",
    "    \n",
    "# remove all other punctuation\n",
    "    text_noNoise = \"\".join([char for char in text_noHash if char not in string.punctuation])\n",
    "\n",
    "    return text_noNoise.lower()\n",
    "\n",
    "\n",
    "df['tweet_noNoise'] = df[\"text\"].apply(lambda x: preprocess_tweets(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tweet_noNoise</th>\n",
       "      <th>lemmatized_list</th>\n",
       "      <th>processed_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>[deed, reason, earthquake, may, allah, forgive...</td>\n",
       "      <td>deed,reason,earthquake,may,allah,forgive,u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>forest,fire,near,la,ronge,sask,canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>[resident, ask, shelter, place, notify, office...</td>\n",
       "      <td>resident,ask,shelter,place,notify,officer,evac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>[13000, people, receive, wildfire, evacuation,...</td>\n",
       "      <td>13000,people,receive,wildfire,evacuation,order...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>[get, send, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "      <td>get,send,photo,ruby,alaska,smoke,wildfire,pour...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                      tweet_noNoise  \\\n",
       "0       1  our deeds are the reason of this earthquake ma...   \n",
       "1       1              forest fire near la ronge sask canada   \n",
       "2       1  all residents asked to shelter in place are be...   \n",
       "3       1  13000 people receive wildfires evacuation orde...   \n",
       "4       1  just got sent this photo from ruby alaska as s...   \n",
       "\n",
       "                                     lemmatized_list  \\\n",
       "0  [deed, reason, earthquake, may, allah, forgive...   \n",
       "1      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2  [resident, ask, shelter, place, notify, office...   \n",
       "3  [13000, people, receive, wildfire, evacuation,...   \n",
       "4  [get, send, photo, ruby, alaska, smoke, wildfi...   \n",
       "\n",
       "                                     processed_tweet  \n",
       "0         deed,reason,earthquake,may,allah,forgive,u  \n",
       "1              forest,fire,near,la,ronge,sask,canada  \n",
       "2  resident,ask,shelter,place,notify,officer,evac...  \n",
       "3  13000,people,receive,wildfire,evacuation,order...  \n",
       "4  get,send,photo,ruby,alaska,smoke,wildfire,pour...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatizer with Part-of-Speech (POS)\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "\n",
    "# this function can be called within lemmatize() to tag the word with its POS\n",
    "def get_pos(word):\n",
    "    probable_part_of_speech = wordnet.synsets(word)\n",
    "  \n",
    "    pos_counts = Counter()\n",
    "\n",
    "    pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "    pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "    pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "    pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "  \n",
    "    most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "    return most_likely_part_of_speech\n",
    "\n",
    "# tokenize tweets = divide into words\n",
    "tokenized_tweets = df.apply(lambda row: tt.tokenize(row['tweet_noNoise']), axis=1)\n",
    "\n",
    "# lemmatize the list of all tokens\n",
    "df['lemmatized_list'] = tokenized_tweets.apply(lambda y: [lemmatizer.lemmatize(x, get_pos(x)) for x in y])\n",
    "df['lemmatized_list'].apply(lambda x: ' '.join([tweet for tweet in x]))\n",
    "\n",
    "# removes stop words (are, on, in, etc.) MUST COME BEFORE LIST -> STR\n",
    "stop = set(stopwords.words('english'))\n",
    "df['lemmatized_list'] = df['lemmatized_list'].apply(lambda x: [y for y in x if y not in stop])\n",
    "\n",
    "# turn the resulting list back into a string\n",
    "df['processed_tweet'] = df.lemmatized_list.agg(lambda x: ','.join(map(str, x)))\n",
    "\n",
    "# remove weird characters and 1 character after\n",
    "df['processed_tweet'] = df['processed_tweet'].replace({'û.':''}, regex=True)\n",
    "df['processed_tweet'] = df['processed_tweet'].replace({'ì.':''}, regex=True)\n",
    "df['processed_tweet'] = df['processed_tweet'].replace({'å.':''}, regex=True)\n",
    "df['processed_tweet'] = df['processed_tweet'].replace({'â.':''}, regex=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weed\n"
     ]
    }
   ],
   "source": [
    "# enter whatever word you'd like to see how lemmatizing works\n",
    "print(lemmatizer.lemmatize('weeds'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         level_0  0000  001  0011  001116  0025  005225  007  00c  00kj  ...  \\\n",
      "0        Tweet 1   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  ...   \n",
      "1        Tweet 2   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  ...   \n",
      "2        Tweet 3   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  ...   \n",
      "3        Tweet 4   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  ...   \n",
      "4        Tweet 5   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  ...   \n",
      "...          ...   ...  ...   ...     ...   ...     ...  ...  ...   ...  ...   \n",
      "7608  Tweet 7609   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  ...   \n",
      "7609  Tweet 7610   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  ...   \n",
      "7610  Tweet 7611   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  ...   \n",
      "7611  Tweet 7612   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  ...   \n",
      "7612  Tweet 7613   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  ...   \n",
      "\n",
      "      zz4  zzaes  zzcb  zzi  zzk  zztb  zzxgh  zzxt  zzzg  zzzz  \n",
      "0     0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "1     0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "2     0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "3     0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "4     0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "...   ...    ...   ...  ...  ...   ...    ...   ...   ...   ...  \n",
      "7608  0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "7609  0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "7610  0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "7611  0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "7612  0.0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "\n",
      "[7613 rows x 23875 columns]\n"
     ]
    }
   ],
   "source": [
    "# first attempts at tf-idf\n",
    "# input needs to exclude mentions & hashtags, be in lower case, remove punct, lemmatized\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(norm=None)\n",
    "tfidf_scores = vectorizer.fit_transform(df.processed_tweet)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# get corpus index\n",
    "corpus_index = [f\"Tweet {i+1}\" for i in range(len(df.processed_tweet))]\n",
    "\n",
    "# create pandas DataFrame with tf-idf scores\n",
    "df_tf_idf = pd.DataFrame(tfidf_scores.T.todense(), index=feature_names, columns=corpus_index)\n",
    "# df_tf_idf.reset_index()\n",
    "\n",
    "df_tf_idf = df_tf_idf.T.reset_index()\n",
    "\n",
    "print(df_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      target\n",
      "0          1\n",
      "1          1\n",
      "2          1\n",
      "3          1\n",
      "4          1\n",
      "...      ...\n",
      "7608       1\n",
      "7609       1\n",
      "7610       1\n",
      "7611       1\n",
      "7612       1\n",
      "\n",
      "[7613 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# put targets into their own dataframe so i can merge them with tf-idf scores\n",
    "target_df = pd.DataFrame()\n",
    "target_df['target'] = df.target\n",
    "print(target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE FOR RIGHT NOW\n",
    "# don't think we need to remove stop words with tf-idf but keep here\n",
    "\n",
    "#stop = set(stopwords.words('english'))\n",
    "\n",
    "# removes 'stop words' such as 'the', 'are', etc. it knows these stop words where i defined 'stop' variable, comes from a library\n",
    "#df['tweet_stop'] = df['lemmatized_tweet'].apply(lambda x: [y for y in x if y not in stop])\n",
    "\n",
    "# hashtags\n",
    "#df['hashtag'] = df.text.apply(lambda x: re.findall(r\"#(\\w+)\", x))\n",
    "\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE FOR RIGHT NOW\n",
    "\n",
    "#df.tweet_stop = df.tweet_stop.apply(lambda x: [' '.join(str(y)) for y in x])\n",
    "#print(df.tweet_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Tweet 1  Tweet 2  Tweet 3  Tweet 4  Tweet 5  Tweet 6  Tweet 7  \\\n",
      "0000          0        0        0        0        0        0        0   \n",
      "001           0        0        0        0        0        0        0   \n",
      "0011          0        0        0        0        0        0        0   \n",
      "001116        0        0        0        0        0        0        0   \n",
      "0025          0        0        0        0        0        0        0   \n",
      "...         ...      ...      ...      ...      ...      ...      ...   \n",
      "zztb          0        0        0        0        0        0        0   \n",
      "zzxgh         0        0        0        0        0        0        0   \n",
      "zzxt          0        0        0        0        0        0        0   \n",
      "zzzg          0        0        0        0        0        0        0   \n",
      "zzzz          0        0        0        0        0        0        0   \n",
      "\n",
      "        Tweet 8  Tweet 9  Tweet 10  ...  Tweet 7605  Tweet 7606  Tweet 7607  \\\n",
      "0000          0        0         0  ...           0           0           0   \n",
      "001           0        0         0  ...           0           0           0   \n",
      "0011          0        0         0  ...           0           0           0   \n",
      "001116        0        0         0  ...           0           0           0   \n",
      "0025          0        0         0  ...           0           0           0   \n",
      "...         ...      ...       ...  ...         ...         ...         ...   \n",
      "zztb          0        0         0  ...           0           0           0   \n",
      "zzxgh         0        0         0  ...           0           0           0   \n",
      "zzxt          0        0         0  ...           0           0           0   \n",
      "zzzg          0        0         0  ...           0           0           0   \n",
      "zzzz          0        0         0  ...           0           0           0   \n",
      "\n",
      "        Tweet 7608  Tweet 7609  Tweet 7610  Tweet 7611  Tweet 7612  \\\n",
      "0000             0           0           0           0           0   \n",
      "001              0           0           0           0           0   \n",
      "0011             0           0           0           0           0   \n",
      "001116           0           0           0           0           0   \n",
      "0025             0           0           0           0           0   \n",
      "...            ...         ...         ...         ...         ...   \n",
      "zztb             0           0           0           0           0   \n",
      "zzxgh            0           0           0           0           0   \n",
      "zzxt             0           0           0           0           0   \n",
      "zzzg             0           0           0           0           0   \n",
      "zzzz             0           0           0           0           0   \n",
      "\n",
      "        Tweet 7613  frequency_summation  \n",
      "0000             0                    1  \n",
      "001              0                    2  \n",
      "0011             0                    1  \n",
      "001116           0                    2  \n",
      "0025             0                    1  \n",
      "...            ...                  ...  \n",
      "zztb             0                    1  \n",
      "zzxgh            0                    1  \n",
      "zzxt             0                    1  \n",
      "zzzg             0                    1  \n",
      "zzzz             0                    1  \n",
      "\n",
      "[23874 rows x 7614 columns]\n"
     ]
    }
   ],
   "source": [
    "# IGNORE FOR RIGHT NOW - THIS IS THE COUNT VECTORIZER FOR IF WE WANT TO REMOVE KEYBOARD SMASHES LATER\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# remove terms that appear only once\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "term_frequencies = vectorizer.fit_transform(df.processed_tweet)\n",
    "\n",
    "# get vocabulary of terms\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "corpus_index = [f\"Tweet {i+1}\" for i in range(len(df.processed_tweet))]\n",
    "\n",
    "# create pandas DataFrame with term frequencies\n",
    "df_term_frequencies = pd.DataFrame(term_frequencies.T.todense(), index=feature_names, columns=corpus_index)\n",
    "\n",
    "df_term_frequencies['frequency_summation'] = df_term_frequencies.iloc[:].sum(axis=1)\n",
    "# df_term_frequencies = df_term_frequencies[df_term_frequencies['frequency_summation'] >= 2]\n",
    "print(df_term_frequencies.iloc[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0000</th>\n",
       "      <th>001</th>\n",
       "      <th>0011</th>\n",
       "      <th>001116</th>\n",
       "      <th>0025</th>\n",
       "      <th>005225</th>\n",
       "      <th>007</th>\n",
       "      <th>00c</th>\n",
       "      <th>00kj</th>\n",
       "      <th>01</th>\n",
       "      <th>...</th>\n",
       "      <th>zz4</th>\n",
       "      <th>zzaes</th>\n",
       "      <th>zzcb</th>\n",
       "      <th>zzi</th>\n",
       "      <th>zzk</th>\n",
       "      <th>zztb</th>\n",
       "      <th>zzxgh</th>\n",
       "      <th>zzxt</th>\n",
       "      <th>zzzg</th>\n",
       "      <th>zzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23874 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0000  001  0011  001116  0025  005225  007  00c  00kj   01  ...  zz4  \\\n",
       "0   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0  ...  0.0   \n",
       "1   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0  ...  0.0   \n",
       "2   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0  ...  0.0   \n",
       "3   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0  ...  0.0   \n",
       "4   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0  ...  0.0   \n",
       "\n",
       "   zzaes  zzcb  zzi  zzk  zztb  zzxgh  zzxt  zzzg  zzzz  \n",
       "0    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
       "1    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
       "2    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
       "3    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
       "4    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 23874 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split up the matrix\n",
    "df_tf_idf.drop(df_tf_idf.columns[0], axis=1, inplace=True)\n",
    "features = df_tf_idf    # feature matrix\n",
    "labels = target_df['target']              # target feature\n",
    "df_tf_idf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the features and the labels:\n",
      "\n",
      "train features:\n",
      "      0000  001  0011  001116  0025  005225  007  00c  00kj   01  ...  zz4  \\\n",
      "5151   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0  ...  0.0   \n",
      "6351   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0  ...  0.0   \n",
      "3443   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0  ...  0.0   \n",
      "7164   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0  ...  0.0   \n",
      "7037   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0  ...  0.0   \n",
      "...    ...  ...   ...     ...   ...     ...  ...  ...   ...  ...  ...  ...   \n",
      "5226   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0  ...  0.0   \n",
      "5390   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0  ...  0.0   \n",
      "860    0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0  ...  0.0   \n",
      "7603   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0  ...  0.0   \n",
      "7270   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0  ...  0.0   \n",
      "\n",
      "      zzaes  zzcb  zzi  zzk  zztb  zzxgh  zzxt  zzzg  zzzz  \n",
      "5151    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "6351    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "3443    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "7164    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "7037    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "...     ...   ...  ...  ...   ...    ...   ...   ...   ...  \n",
      "5226    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "5390    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "860     0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "7603    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "7270    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "\n",
      "[5709 rows x 23874 columns]\n",
      "test features:\n",
      "      0000  001  0011  001116  0025  005225  007  00c  00kj   01  ...  zz4  \\\n",
      "2644   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0  ...  0.0   \n",
      "2227   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0  ...  0.0   \n",
      "5448   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0  ...  0.0   \n",
      "132    0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0  ...  0.0   \n",
      "6845   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0  ...  0.0   \n",
      "...    ...  ...   ...     ...   ...     ...  ...  ...   ...  ...  ...  ...   \n",
      "5209   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0  ...  0.0   \n",
      "387    0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0  ...  0.0   \n",
      "4848   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0  ...  0.0   \n",
      "1032   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0  ...  0.0   \n",
      "7195   0.0  0.0   0.0     0.0   0.0     0.0  0.0  0.0   0.0  0.0  ...  0.0   \n",
      "\n",
      "      zzaes  zzcb  zzi  zzk  zztb  zzxgh  zzxt  zzzg  zzzz  \n",
      "2644    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "2227    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "5448    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "132     0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "6845    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "...     ...   ...  ...  ...   ...    ...   ...   ...   ...  \n",
      "5209    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "387     0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "4848    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "1032    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "7195    0.0   0.0  0.0  0.0   0.0    0.0   0.0   0.0   0.0  \n",
      "\n",
      "[1904 rows x 23874 columns]\n",
      "train labels:\n",
      "5151    0\n",
      "6351    0\n",
      "3443    0\n",
      "7164    1\n",
      "7037    1\n",
      "       ..\n",
      "5226    0\n",
      "5390    0\n",
      "860     0\n",
      "7603    1\n",
      "7270    1\n",
      "Name: target, Length: 5709, dtype: int64\n",
      "test labels:\n",
      "2644    1\n",
      "2227    0\n",
      "5448    1\n",
      "132     0\n",
      "6845    0\n",
      "       ..\n",
      "5209    0\n",
      "387     1\n",
      "4848    1\n",
      "1032    0\n",
      "7195    1\n",
      "Name: target, Length: 1904, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)\n",
    "\n",
    "print('These are the features and the labels:\\n')\n",
    "print('train features:')\n",
    "print(train_features)\n",
    "print('test features:')\n",
    "print(test_features)\n",
    "print('train labels:')\n",
    "print(train_labels)\n",
    "print('test labels:')\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapper method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper method 1 with k range - selma\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "sfs = SFS(RandomForestClassifier(), \n",
    "           k_features=(3, 15),\n",
    "           forward=True, \n",
    "           floating=False, \n",
    "           scoring='accuracy',\n",
    "           cv=5)\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), sfs)\n",
    "\n",
    "pipe.fit(train_features, train_labels)\n",
    "\n",
    "print('best combination (ACC: %.3f): %s\\n' % (sfs.k_score_, sfs.k_feature_idx_))\n",
    "sfs.k_feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best combination (ACC: 0.000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "STOPPING EARLY DUE TO KEYBOARD INTERRUPT..."
     ]
    }
   ],
   "source": [
    "# Wrapper method 2 - selma\n",
    "sfs = SFS(RandomForestClassifier(),\n",
    "          k_features=2828,\n",
    "          forward=True,\n",
    "          floating=False,\n",
    "          scoring = 'accuracy',\n",
    "          cv = 0)\n",
    "\n",
    "sfs.fit(train_features, train_labels)\n",
    "\n",
    "#print('best combination (ACC: %.3f): %s\\n' % (sfs.k_score_, sfs.k_feature_idx_))\n",
    "print('best combination (ACC: %.3f)' % (sfs.k_score_))\n",
    "#sfs.k_feature_names_     # to get the final set of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the constants, does not run, first fix data above\n",
    "#I think tweets must be preprocessed first though\n",
    "constant_features = [var for var in train_features.columns if train_features[var].std() == 0] \n",
    "\n",
    "no_constant_train = train_features.drop(labels=constant_features, axis=1, inplace=True)\n",
    "no_constant_test = test_features.drop(labels=constant_features, axis=1, inplace=True) \n",
    " \n",
    "\n",
    "print(no_constant_train)\n",
    "#no_constant_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got scalar array instead:\narray=nan.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-5167dd48eefa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mquasi_remover\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVarianceThreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Find the values with low variance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mquasi_remover\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_constant_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquasi_remover\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Apply to datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/feature_selection/_variance_threshold.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mitself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \"\"\"\n\u001b[0;32m---> 91\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0;31m# If input is scalar raise error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    762\u001b[0m                     \u001b[0;34m\"Expected 2D array, got scalar array instead:\\narray={}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got scalar array instead:\narray=nan.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "#Removing Quasi-constant features,does not run\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest\n",
    "\n",
    "# Define the threshold as 0.01\n",
    "quasi_remover = VarianceThreshold(threshold=0.01)\n",
    "# Find the values with low variance\n",
    "quasi_remover.fit(no_constant_train) \n",
    "sum(quasi_remover.get_support())\n",
    "# Apply to datasets\n",
    "no_quasi_train = quasi_remover.transform(no_constant_train)\n",
    "no_quasi_test = quasi_remover.transform(no_constant_test)\n",
    "\n",
    "no_quasi_train.shape, no_quasi_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-d53e3cad5611>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_dupl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mno_dupl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;31m#, X_test.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "#REMOVE DUPLICATE FEATURES\n",
    "duplFeatures = []\n",
    "for i in range(0, len(train_features.columns)):\n",
    "    oneCol = train_features.columns[i]\n",
    "for othCol in train_features.columns[i + 1:]:\n",
    "    if train_features[oneCol].equals(train_features[othCol]):\n",
    "            duplFeatures.append(othCol)\n",
    "            \n",
    "no_dupl = train_features.drop(labels=duplFeatures, axis=1, inplace=True)\n",
    "#X_test.drop(labels=duplFeatures, axis=1, inplace=True)\n",
    "\n",
    "print(no_dupl)\n",
    "no_dupl.shape #, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVE CORRELATED FEATURES\n",
    "correl_Feat = set() \n",
    "correl_matrix = train_features.corr()\n",
    "    \n",
    "for i in range(len(corr_matrix.columns)):\n",
    "   for j in range(i):\n",
    "       if abs(correl_matrix.iloc[i, j]) > 0.8:\n",
    "           colName = correl_matrix.columns[i]  \n",
    "           correl_Feat.add(colname)\n",
    "            \n",
    "no_correl = train_features.drop(labels=correl_Feat, axis=1, inplace=True)\n",
    "\n",
    "print(no_correl)\n",
    "no_correl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after each filter or removing of features,\n",
    "#a copy can be made and stored to later compare performance. We can check performance of original list,\n",
    "#vs after removing correlated vs after removing quasi constants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR WRAPPER METHOD - GENERATE THE NEW TRAIN AND TEST DATAFRAMES BASED ON SELECTED FEATURES\n",
    "\n",
    "# Note that the transform call is equivalent to\n",
    "# features_train[:, sfs.k_feature_idx_]\n",
    "\n",
    "features_train_sfs = sfs.transform(X_train)\n",
    "features_test_sfs = sfs.transform(X_test)\n",
    "print(features_train_sfs)\n",
    "print(features_test_sfs)\n",
    "\n",
    "# Fit the estimator using the new feature subset\n",
    "# and make a prediction on the test data\n",
    "model = RandomForestClassifier()\n",
    "model.fit(features_train_sfs, labels_train)\n",
    "labels_pred = model.predict(features_test_sfs)\n",
    "\n",
    "# Compute the accuracy of the prediction\n",
    "acc = float((labels_test == labels_pred).sum()) / labels_pred.shape[0]\n",
    "print('Test set accuracy: %.2f %%' % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest - train the model and test, report score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "print('\\nThis is RandomForest score:')\n",
    "model = RandomForestClassifier()\n",
    "model.fit(train_features, train_labels)\n",
    "print(model.score(test_features, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression - train the model and test, report score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "print('\\nThis is Linear Regression score:')\n",
    "model2 = LinearRegression()\n",
    "model2.fit(train_features, train_labels)\n",
    "model2.score(test_features, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
